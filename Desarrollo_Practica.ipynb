{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Aplicación de CNN para reconocimiento de Pokémon y generación de una interfaz explicativa</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Generales.\n",
    "\n",
    "* Asignatura: Inteligencia Artificial en las Organizaciones\n",
    "* Curso: 2023/2024\n",
    "* Profesor: Agapito Ismael Ledezma Espino\n",
    "* Grupo: 85\n",
    "\n",
    "Alumnos: \n",
    "- Jonathan Jiménez Muñoz (100451132@alumnos.uc3m.es)\n",
    "- Marta Palomo Velasco (100451041@alumnos.uc3m.es)\n",
    "- Francisco Antonio Gallardo Fuentes (100451146@alumnos.uc3m.es)\n",
    "- Yago Brotón Gutiérrez (100451322@alumnos.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El objetivo de está práctica va a ser utilizar Redes Neuronales Convolucionales para clasificar imágenes de Pokémon y, una vez identificado dicho Pokémon, construir una pequeña interfaz donde se muestren estadísticas relevantes sobre el mismo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "Cada año salen más y más Pokemon, habiendo actualmente más de 1000 especies diferentes, donde cada especie tiene n movimientos diferentes, 2/3 habilidades diferentes, naturalezas, tipos, etc. <br><br>\n",
    "Es por ello que las personas junior que intentan competir profesionalmente en el juego de Pokemon, se ven abrumadas por la cantidad de información que tienen que procesar, y por ello cada vez hay más y más jugadores masters (+18 años) y menos senior y junior (menos de 18 años), dado que los jugadores que antes eran junior han cambiado de categoría y cada vez hay menos jugadores nuevos que se unan a la comunidad. <br><br>\n",
    "Con esta práctica lo que queremos lograr es que, a partir de una simple imagen de un Pokemon, se muestre información relevante sobre el mismo, como por ejemplo, sus estadísticas base, sus habilidades, sus movimientos, etc, y con ello que los jugadores junior puedan aprender más rápido y tener algo más de motivación para entrar en la comunidad competitiva de Pokemon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo de la práctica\n",
    "\n",
    "Esta práctica va a estar dividida en varias fases:\n",
    "\n",
    "1. Obtención de los datos: En esta fase vamos a obtener aquellas imágenes que vamos a utilizar para entrenar nuestro modelo, y vamos a preprocesarlas para que el modelo pueda entrenar con ellas. Además de obtener los datos usados para la interfaz. <br><br>\n",
    "2. Preprocesado de los datos: Una vez obtengamos todos los datos/imágenes posibles, tendremos que hacer una limpieza (eliminar duplicados, imágenes que no correspondan al Pokemon que queremos, etc), con el objetivo de tener un dataset lo más limpio posible.<br><br>\n",
    "3. División train_test y últimas preparaciones para el modelo: Tras haber preprocesado los datos, el siguiente paso es dividir el dataset en train y test, y realizar las últimas preparaciones para el modelo (generación de más imágenes, etc).<br><br>\n",
    "4. Construcción del modelo: Una vez tenemos los datos preparados, el siguiente paso es construir el modelo. Para ello, vamos a utilizar una red neuronal convolucional CNN, que es un tipo de red neuronal que se utiliza para clasificar imágenes. <br><br>\n",
    "5. Desarrollo de la Interfaz: Finalmente, con el modelo ya entrenado y seleccionado, tendremos que desarrollar una interfaz que muestre aquella información (mejores movimientos, habilidades, estadísticas, etc) que consideremos relevantes para el jugador y que le ayuden a mejorar en el juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtención de los datos\n",
    "\n",
    "Para obtener las imágenes y datos sobre los Pokémon, hemos utilizado diferentes fuentes de información, como por ejemplo:\n",
    "Kaggle: \n",
    "- https://www.kaggle.com/datasets/abcsds/pokemon\n",
    "- https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types\n",
    "- https://www.kaggle.com/datasets/kvpratama/pokemon-images-dataset\n",
    "- https://www.kaggle.com/datasets/arenagrenade/the-complete-pokemon-images-data-set\n",
    "\n",
    "ImagenDex:\n",
    "- https://pokemaster.es/imagendex/\n",
    "\n",
    "Una vez hemos obtenido imágenes de todos los Pokémon, el próximo objetivo es escoger aquellos que más imágenes tenemos y a partir de ahí, obtener más imágenes de los mismos. <br>\n",
    "\n",
    "Para ello decidimos crear un pequeño script que nos permitiera obtener imágenes a partir de Google Imágenes. <br>\n",
    "\n",
    "En dicho script utilizamos las librerías: google_search, para realizar búsquedas en Google Imágenes; requests, para realizar peticiones a las páginas web y BeautifulSoup para parsear el HTML de las páginas web y obtener las imágenes. <br>\n",
    "\n",
    "En caso de requerir de más imágenes, a la hora de generar el modelo utilizaremos algunas técnicas como poner la imagen en modo espejo, rotarla, etc y así obtener hacer que nuestro modelo sea lo más preciso posible. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom googlesearch import search\\n\\n# Función para la descarga de imágenes\\ndef download_all_images(query):\\n    image_links = set()\\n\\n    # Searching for the query in Google Images\\n    search_query = query + \" images\"\\n    for j in search(search_query, num=10, stop=10, pause=2):\\n        if \\'https://encrypted-tbn0.gstatic.com/images\\' in j:\\n            continue\\n        page = requests.get(j)\\n        soup = BeautifulSoup(page.content, \"html.parser\")\\n\\n        # Extracting image links\\n        for raw_img in soup.find_all(\"img\"):\\n            link = raw_img.get(\"src\")\\n            if link and link.startswith(\"http\"):\\n                image_links.add(link)\\n\\n    # Create a directory for downloaded images\\n    if not os.path.exists(query):\\n        os.makedirs(query)\\n\\n    # Download the images\\n    for i, link in enumerate(image_links):\\n        try:\\n            response = requests.get(link)\\n            file = open(os.path.join(query, f\"{query}_{i+1}.jpg\"), \"wb\")\\n            file.write(response.content)\\n            file.close()\\n        except Exception as e:\\n            print(f\"Error: {e}\")\\n            continue\\n    \\n    print(f\"Downloaded {len(image_links)} images for {query}.\")\\n    return\\n\\n\\nquery = \"squirtle in game\"  # Introducir el término aquí.\\n\\ndownload_all_images(query)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la búsqueda de imágenes en Google imágenes (descomentar si se quiere probar)\n",
    "\n",
    "# Importamos las librerías necesarias\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "\n",
    "# Función para la descarga de imágenes\n",
    "def download_all_images(query):\n",
    "    image_links = set()\n",
    "\n",
    "    # Searching for the query in Google Images\n",
    "    search_query = query + \" images\"\n",
    "    for j in search(search_query, num=10, stop=10, pause=2):\n",
    "        if 'https://encrypted-tbn0.gstatic.com/images' in j:\n",
    "            continue\n",
    "        page = requests.get(j)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # Extracting image links\n",
    "        for raw_img in soup.find_all(\"img\"):\n",
    "            link = raw_img.get(\"src\")\n",
    "            if link and link.startswith(\"http\"):\n",
    "                image_links.add(link)\n",
    "\n",
    "    # Create a directory for downloaded images\n",
    "    if not os.path.exists(query):\n",
    "        os.makedirs(query)\n",
    "\n",
    "    # Download the images\n",
    "    for i, link in enumerate(image_links):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            file = open(os.path.join(query, f\"{query}_{i+1}.jpg\"), \"wb\")\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Downloaded {len(image_links)} images for {query}.\")\n",
    "    return\n",
    "\n",
    "\n",
    "query = \"squirtle in game\"  # Introducir el término aquí.\n",
    "\n",
    "download_all_images(query)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargadas las imágenes, vamos a escribir un nombre en cada imagen igual, para que después sea más sencillo el preprocesado de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Importar librerías\\nimport os\\n\\npokemon = 'Squirtle' # Cambiar para cada nombre de pokemon\\npath = 'pokemon/'+pokemon\\n\\n# Recorremos las imágenes de la carpeta y vamos cambiando el nombre\\ni = 0\\nfor filename in os.listdir(path):\\n    os.rename(path+'/'+filename, path+'/'+pokemon+'__'+str(i)+'.png')\\n    i += 1\\n    \\nprint('Nombres cambiados')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para cambiar el nombre de las imágenes descargadas (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "# Importar librerías\n",
    "import os\n",
    "\n",
    "pokemon = 'Squirtle' # Cambiar para cada nombre de pokemon\n",
    "path = 'pokemon/'+pokemon\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el nombre\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    os.rename(path+'/'+filename, path+'/'+pokemon+'__'+str(i)+'.png')\n",
    "    i += 1\n",
    "    \n",
    "print('Nombres cambiados')\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo esto, obtuvimos muchas imágenes para cada Pokémon. Sin embargo, decidimos quedarnos con los Pokémon que más imágenes tenían, debido a que consideramos que es mejor obtener un buen modelo que funcione para una cantidad reducida de especies, que crear un modelo que funcione para muchas especies pero no sea demasiado preciso. <br>\n",
    "\n",
    "Por ello, los Pokémon que decidimos utilizar fueron los siguientes: Pikachu, Charmander, Charizard, Caterpie, Magikarp, Ratata, Geodude, Machop, Squirtle, Bulbasaur, Mew, Dragonite, Meowth, Lapras, Snorlax, Greninja, Rayquaza, Lucario, MrMime y Gengar (20 especies). <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesado de los datos\n",
    "Debido a que utilizamos diferentes filtros en la búsqueda de imágenes para obtener las máximas posibles, nos salieron muchas imágenes duplicadas o que no se correspondían con el Pokémon que queríamos. <br>\n",
    "\n",
    "Es por ello que debíamos hacer un preprocesado de los datos, para quedarnos con el conjunto de imágenes de cada Pokémon lo más limpio y homogéneo posible. <br>\n",
    "\n",
    "Esto conyevó varias tareas: \n",
    "- Eliminación de imágenes no correspondientes al Pokémon que queríamos. Para ello simplemente una vez descargadas todas las imágenes, nos metíamos en la carpeta e íbamos eliminando una a una las imágenes que no pertenecían a dicho Pokémon.\n",
    "- Eliminación de imágenes duplicadas. En este punto nos dimos cuenta de que hacerlo a mano iba a ser prácticamente imposible, por ello decidimos crear un pequeño script que comparase las imágenes entre sí y eliminase aquellas que fueran iguales, usando la libería Pilow.\n",
    "- Normalizar, redimensionar y convertir las imágenes a escala de grises. Para ello, utilizaremos keras.preprocessing.image, que nos permite realizar estas tareas de forma muy sencilla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Scripts usados para el preprocesdo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nimport os\\n\\n# Ruta de la carpeta con las imágenes\\ncarpeta = \\'pokemon/Meowth\\'\\n\\n# Diccionario para almacenar las imágenes idénticas\\nimagenes_ident = {}\\n\\n# Recorriendo la carpeta y comparando cada imagen con las demás\\nfor nombre_imagen1 in os.listdir(carpeta):\\n    ruta_imagen1 = os.path.join(carpeta, nombre_imagen1)\\n    if os.path.isfile(ruta_imagen1):\\n        imagen1 = Image.open(ruta_imagen1)\\n        for nombre_imagen2 in os.listdir(carpeta):\\n            ruta_imagen2 = os.path.join(carpeta, nombre_imagen2)\\n            if os.path.isfile(ruta_imagen2) and ruta_imagen1 != ruta_imagen2:\\n                imagen2 = Image.open(ruta_imagen2)\\n                if imagen1.size == imagen2.size and list(imagen1.getdata()) == list(imagen2.getdata()):\\n                    if nombre_imagen1 not in imagenes_ident:\\n                        imagenes_ident[nombre_imagen1] = [nombre_imagen2]\\n                    else:\\n                        imagenes_ident[nombre_imagen1].append(nombre_imagen2)\\n\\n# Mostrar las imágenes idénticas encontradas\\nfor imagen, imagenes_iguales in imagenes_ident.items():\\n    print(f\"La imagen {imagen} es idéntica a: {\\', \\'.join(imagenes_iguales)}\")\\n    \\n# Sacamos la imagen duplicada de la carpeta y la movemos a otra carpeta, dejando solamente una copia de la imagen\\nfor imagen, imagenes_iguales in imagenes_ident.items():\\n    # si no existe la carpeta la creamos\\n    if not os.path.exists(\\'pokemon/duplicadas\\'):\\n        os.makedirs(\\'pokemon/duplicadas\\')\\n    # movemos la imagen a la carpeta duplicadas\\n    os.rename(carpeta + \\'/\\' + imagen, \\'pokemon/duplicadas/\\' + imagen)\\n    # borramos las imagenes duplicadas\\n    for imagen_duplicada in imagenes_iguales:\\n        os.remove(carpeta + \\'/\\' + imagen_duplicada)\\n     \\n    \\n# Si no hay imágenes idénticas, mostrar un mensaje\\nif len(imagenes_ident) == 0:\\n    print(\"No hay imágenes idénticas.\")\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script de eliminación de imágenes duplicadas (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Ruta de la carpeta con las imágenes\n",
    "carpeta = 'pokemon/Meowth'\n",
    "\n",
    "# Diccionario para almacenar las imágenes idénticas\n",
    "imagenes_ident = {}\n",
    "\n",
    "# Recorriendo la carpeta y comparando cada imagen con las demás\n",
    "for nombre_imagen1 in os.listdir(carpeta):\n",
    "    ruta_imagen1 = os.path.join(carpeta, nombre_imagen1)\n",
    "    if os.path.isfile(ruta_imagen1):\n",
    "        imagen1 = Image.open(ruta_imagen1)\n",
    "        for nombre_imagen2 in os.listdir(carpeta):\n",
    "            ruta_imagen2 = os.path.join(carpeta, nombre_imagen2)\n",
    "            if os.path.isfile(ruta_imagen2) and ruta_imagen1 != ruta_imagen2:\n",
    "                imagen2 = Image.open(ruta_imagen2)\n",
    "                if imagen1.size == imagen2.size and list(imagen1.getdata()) == list(imagen2.getdata()):\n",
    "                    if nombre_imagen1 not in imagenes_ident:\n",
    "                        imagenes_ident[nombre_imagen1] = [nombre_imagen2]\n",
    "                    else:\n",
    "                        imagenes_ident[nombre_imagen1].append(nombre_imagen2)\n",
    "\n",
    "# Mostrar las imágenes idénticas encontradas\n",
    "for imagen, imagenes_iguales in imagenes_ident.items():\n",
    "    print(f\"La imagen {imagen} es idéntica a: {', '.join(imagenes_iguales)}\")\n",
    "    \n",
    "# Sacamos la imagen duplicada de la carpeta y la movemos a otra carpeta, dejando solamente una copia de la imagen\n",
    "for imagen, imagenes_iguales in imagenes_ident.items():\n",
    "    # si no existe la carpeta la creamos\n",
    "    if not os.path.exists('pokemon/duplicadas'):\n",
    "        os.makedirs('pokemon/duplicadas')\n",
    "    # movemos la imagen a la carpeta duplicadas\n",
    "    os.rename(carpeta + '/' + imagen, 'pokemon/duplicadas/' + imagen)\n",
    "    # borramos las imagenes duplicadas\n",
    "    for imagen_duplicada in imagenes_iguales:\n",
    "        os.remove(carpeta + '/' + imagen_duplicada)\n",
    "     \n",
    "    \n",
    "# Si no hay imágenes idénticas, mostrar un mensaje\n",
    "if len(imagenes_ident) == 0:\n",
    "    print(\"No hay imágenes idénticas.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Importar librerías\\nimport os\\nfrom PIL import Image\\n\\n# Seleccionamos la carpeta con las imágenes a normalizar\\npokemon = \\'Pikachu\\' # Cambiar para cada nombre de pokemon\\npath = \\'pokemon/\\'+pokemon\\n\\n# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\\ni = 0\\nfor filename in os.listdir(path):\\n    # Normalizar\\n    img = Image.open(path+\\'/\\'+filename)\\n    img = img.resize((128,128))\\n    img.save(path+\\'/\\'+pokemon+\\'_\\'+str(i)+\\'.png\\')\\n    i += 1\\n    \\nprint(\\'Imágenes normalizadas\\')\\n\\n# Eliminar todos los jpg para evitar duplicados en varios formatos\\n\\nfor filename in os.listdir(path):\\n    if filename.endswith(\".jpg\"):\\n        os.remove(path+\\'/\\'+filename)\\n        \\nprint(\\'Imágenes jpg eliminadas\\')\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la normalización (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Seleccionamos la carpeta con las imágenes a normalizar\n",
    "pokemon = 'Pikachu' # Cambiar para cada nombre de pokemon\n",
    "path = 'pokemon/'+pokemon\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    # Normalizar\n",
    "    img = Image.open(path+'/'+filename)\n",
    "    img = img.resize((128,128))\n",
    "    img.save(path+'/'+pokemon+'_'+str(i)+'.png')\n",
    "    i += 1\n",
    "    \n",
    "print('Imágenes normalizadas')\n",
    "\n",
    "# Eliminar todos los jpg para evitar duplicados en varios formatos\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        os.remove(path+'/'+filename)\n",
    "        \n",
    "print('Imágenes jpg eliminadas')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Importar librerías\\nimport os\\nimport cv2\\nimport numpy as np\\n\\n# Seleccionamos la carpeta con las imágenes a cambiar\\npokemon = 'MrMime' # Cambiar para cada nombre de pokemon\\npath = 'pokemon/'+pokemon\\n\\n# Recorremos las imágenes de la carpeta y vamos aplicando el filtro\\ni = 0\\nfor filename in os.listdir(path):\\n    # Normalizar\\n    img = cv2.imread(path+'/'+filename)\\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    cv2.imwrite(path+'/'+pokemon+'_'+str(i)+'.png',gray)\\n    i += 1\\n    \\nprint('Imágenes en escala de grises')\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la conversión a escala de grises (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Seleccionamos la carpeta con las imágenes a cambiar\n",
    "pokemon = 'MrMime' # Cambiar para cada nombre de pokemon\n",
    "path = 'pokemon/'+pokemon\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos aplicando el filtro\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    # Normalizar\n",
    "    img = cv2.imread(path+'/'+filename)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(path+'/'+pokemon+'_'+str(i)+'.png',gray)\n",
    "    i += 1\n",
    "    \n",
    "print('Imágenes en escala de grises')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos anteriormente, a pesar de haber realizado una búsqueda masiva de imágenes, creemos que con apenas 400-1000 imágenes de cada especie no son suficientes, es por ello que vamos a utilizar técnicas como la rotación o volteo y así obtener, como mínimo, 2000 imágenes de cada especie. Para ello, podemos hacerlo de dos maneras:\n",
    "1. Realizarlo directamente sobre el conjunto de imágenes.\n",
    "2. Realizar la división del conjunto de imágenes en train y test y aplicar las técnicas de rotación y volteo sobre el conjunto de train.\n",
    "\n",
    "Cualquiera de las opciones debería ser correcta, pero hemos decidido realizar la segunda opción, ya que así podemos usar directamente el conjunto de train a la hora de crear el modelo y no tendríamos que hacer una división posteriormente, es decir, nos ahorra un paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Generamos 3000 imágenes de cada especie a partir de las 400-1000 que teníamos.\\n\\n# Importamos librerías\\nfrom keras.preprocessing.image import ImageDataGenerator\\nimport random\\n\\n# Generador de imágenes\\ngenerador_imagenes = ImageDataGenerator(rotation_range=10, # Rotación aleatoria de la imagen\\n                                        width_shift_range=0.1, # Desplazamiento horizontal aleatorio\\n                                        height_shift_range=0.1, # Desplazamiento vertical aleatorio\\n                                        zoom_range=0.1, # Zoom aleatorio\\n                                        horizontal_flip=True) # Volteo horizontal\\n\\n# Generamos 50 imágenes de cada imagen que tenemos y se guardan en la carpeta de la clase correspondiente\\nfor elemento in clases:\\n    # Recorremos las imágenes de cada clase\\n    for filename in os.listdir(path+'/'+elemento):\\n        # Añadimos la imagen a la lista X\\n        img = cv2.imread(path+'/'+elemento+'/'+filename)\\n        # Revisar que todas las imágenes tienen el mismo tamaño\\n        img = cv2.resize(img, (128,128))\\n        # Cambiamos la dimensión de la imagen\\n        img = img.reshape([-1, 128, 128, 3])\\n        # Generamos las imágenes\\n        generador_imagenes.fit(img)\\n        imagenes_generadas = generador_imagenes.flow(img, batch_size=50, save_to_dir=path+'/'+elemento, save_prefix='aug', save_format='png')\\n        # Guardamos las imágenes\\n        for i in range(10):\\n            imagenes_generadas.next()\\n            \\nprint('Imágenes generadas')\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generamos más imágenes (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "# Generamos 3000 imágenes de cada especie a partir de las 400-1000 que teníamos.\n",
    "\n",
    "# Importamos librerías\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "\n",
    "# Generador de imágenes\n",
    "generador_imagenes = ImageDataGenerator(rotation_range=10, # Rotación aleatoria de la imagen\n",
    "                                        width_shift_range=0.1, # Desplazamiento horizontal aleatorio\n",
    "                                        height_shift_range=0.1, # Desplazamiento vertical aleatorio\n",
    "                                        zoom_range=0.1, # Zoom aleatorio\n",
    "                                        horizontal_flip=True) # Volteo horizontal\n",
    "\n",
    "# Generamos 50 imágenes de cada imagen que tenemos y se guardan en la carpeta de la clase correspondiente\n",
    "for elemento in clases:\n",
    "    # Recorremos las imágenes de cada clase\n",
    "    for filename in os.listdir(path+'/'+elemento):\n",
    "        # Añadimos la imagen a la lista X\n",
    "        img = cv2.imread(path+'/'+elemento+'/'+filename)\n",
    "        # Revisar que todas las imágenes tienen el mismo tamaño\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        # Cambiamos la dimensión de la imagen\n",
    "        img = img.reshape([-1, 128, 128, 3])\n",
    "        # Generamos las imágenes\n",
    "        generador_imagenes.fit(img)\n",
    "        imagenes_generadas = generador_imagenes.flow(img, batch_size=50, save_to_dir=path+'/'+elemento, save_prefix='aug', save_format='png')\n",
    "        # Guardamos las imágenes\n",
    "        for i in range(10):\n",
    "            imagenes_generadas.next()\n",
    "            \n",
    "print('Imágenes generadas')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, conseguimos tener un dataset lo más limpio posible y preparado para el siguiente punto: realización del modelo y entrenamiento <directorio ./Pokemon>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar con la creación del modelo, vamos a visualizar a través de gráficas las diferentes clases que tenemos en nuestro dataset, con el objetivo de ver el número de imágenes que tenemos para cada clase, si hay desbalanceo o no. Recordamos que como tenemos 20 pokémon, el número de clases será 20. <br>\n",
    "Para la realización de este modelo, nos hemos basado en páginas de referencia tales como: stackoverflow, tensorflow y keras; en los cuales hemos usado el código que nos proporcionan, realizando las modificaciones necesarias para adaptarlo a nuestro problema. <br>\n",
    "- TensorFlow: https://www.tensorflow.org/tutorials/images/classification\n",
    "- Keras: https://keras.io/examples/vision/image_classification_from_scratch/\n",
    "- Stackoverflow: usado para resolver dudas puntuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de clases:  20\n",
      "Número de imágenes por clase:  {'Bulbasaur': 313, 'Caterpie': 173, 'Charizard': 424, 'Charmander': 680, 'Dragonite': 461, 'Eevee': 340, 'Gengar': 334, 'Geodude': 218, 'Greninja': 308, 'Jigglypuff': 503, 'Lapras': 417, 'Machop': 184, 'Magikarp': 373, 'Meowth': 576, 'MrMime': 293, 'Pikachu': 853, 'Rattata': 49, 'Rayquaza': 433, 'Snorlax': 456, 'Squirtle': 521}\n",
      "Número de clases:  5\n",
      "Número de imágenes por clase:  {'Pikachu': 853, 'Charmander': 680, 'Meowth': 576, 'Squirtle': 521, 'Jigglypuff': 503}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Script para pre-visualizar las clases y el número de imágenes por clase (descomentar si se quiere probar)\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionamos la carpeta con las diferentes clases\n",
    "path = 'pokemon'\n",
    "clases = os.listdir(path) # Lista con las clases\n",
    "print(\"Número total de clases: \", len(clases))\n",
    "\n",
    "# Creamos un diccionario para almacenar el número de imágenes por clase\n",
    "num_imagenes = {}\n",
    "for elemento in clases:\n",
    "    num_imagenes[elemento] = len(os.listdir(path+'/'+elemento))\n",
    "    \n",
    "print(\"Número de imágenes por clase: \", num_imagenes)\n",
    "\n",
    "# cogemos solo las 5 clases con más imágenes\n",
    "num_imagenes = {k: v for k, v in sorted(num_imagenes.items(), key=lambda item: item[1], reverse=True)[:5]}\n",
    "clases = list(num_imagenes.keys())\n",
    "print(\"Número de clases: \", len(clases))\n",
    "print(\"Número de imágenes por clase: \", num_imagenes)\n",
    "\n",
    "# Dibujamos la gráfica\n",
    "plt.bar(range(len(num_imagenes)), list(num_imagenes.values()), align='center')\n",
    "plt.xticks(range(len(num_imagenes)), list(num_imagenes.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes:  3133\n",
      "Número de etiquetas:  3133\n",
      "Forma de X_train:  (2506, 128, 128, 3)\n",
      "Forma de X_test:  (627, 128, 128, 3)\n",
      "Forma de Y_train:  (2506,)\n",
      "Forma de Y_test:  (627,)\n"
     ]
    }
   ],
   "source": [
    "# Definimos el Conjunto X e Y, donde X son las imágenes y Y las clases\n",
    "\n",
    "# Importar librerías\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "X = [] # Lista con las imágenes\n",
    "Y = [] # Lista de etiquetas\n",
    "\n",
    "# Recorremos las clases\n",
    "for elemento in clases:\n",
    "    # Recorremos las imágenes de cada clase\n",
    "    for filename in os.listdir(path+'/'+elemento):\n",
    "        # Añadimos la imagen a la lista X\n",
    "        img = cv2.imread(path+'/'+elemento+'/'+filename)\n",
    "        # Revisar que todas las imágenes tienen el mismo tamaño\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        X.append(img)\n",
    "        Y.append(elemento)\n",
    "        \n",
    "# Imprimimos la longitud de las listas\n",
    "print(\"Número de imágenes: \", len(X))\n",
    "print(\"Número de etiquetas: \", len(Y))\n",
    "\n",
    "# Convertimos las listas a arrays de numpy\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    " \n",
    "# División del conjunto de datos en entrenamiento y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=22, stratify=Y, shuffle=True)\n",
    "print(\"Forma de X_train: \", X_train.shape)\n",
    "print(\"Forma de X_test: \", X_test.shape)\n",
    "print(\"Forma de Y_train: \", Y_train.shape)\n",
    "print(\"Forma de Y_test: \", Y_test.shape)\n",
    "\n",
    "# Codificación one hot de las etiquetas\n",
    "encoder = LabelEncoder() \n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "Y_test = encoder.transform(Y_test)\n",
    "Y_train = to_categorical(Y_train, num_classes=5)\n",
    "Y_test = to_categorical(Y_test, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos antes, vamos a generar más imágenes a aprtir de técnicas como la rotación, modo espejo, zoom, etc. Para ello, vamos a utilizar la librería ImageDataGenerator de keras, que nos permite realizar estas técnicas de forma muy sencilla. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "generador_imagenes = ImageDataGenerator(rotation_range = 45, # Graus de rotación\n",
    "                            zoom_range = 0.2, # Zoom a aplicar\n",
    "                            horizontal_flip = True, # Volteo horizontal\n",
    "                            width_shift_range = 0.15, # Desplazamiento horizontal\n",
    "                            height_shift_range = 0.15, # Desplazamiento vertical\n",
    "                            shear_range = 0.2) # Cizallamiento\n",
    "\n",
    "generador_imagenes.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construcción del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos ya el conjunto de datos dividido en train and test, todas las imágenes necesarias generadas y el dataset limpio y preparado, vamos a proceder a la creación del modelo CNN. <br>\n",
    "Para ello seguiremos utilizando la librerçia Keras, la cual nos parece muy útil para realizar esta construcción y, además, contiene muchos ejemplos reales y en los cuales podemos orientarnos a la hora de realizar nuestro modelo. <br>\n",
    "A la hora de su realización, podemos basarnos en varios tipos de enfoque: <br> \n",
    "1. Secuencial: se trata de una pila de capas de redes neuronales, en la cual cada capa tiene una entrada y una salida: https://keras.io/guides/sequential_model/ <br>\n",
    "2. Funcional: se trata de un modelo más complejo, en el cual podemos tener varias entradas y salidas, y además podemos tener capas compartidas: https://keras.io/guides/functional_api/<br>\n",
    "\n",
    "Nosotros hemos decidido utilizar el modelo secuencial, ya que consideramos que es más sencillo y más adecuado para nuestro problema dado que como lo que queremos hacer es que, dada una imagen se genere una interfaz que nos diga que Pokémon es, solo necesitamos de una entrada y una salida. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_59 (Conv2D)          (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_54 (Ba  (None, 128, 128, 32)      128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPooli  (None, 64, 64, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_55 (Ba  (None, 64, 64, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_56 (Ba  (None, 64, 64, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPooli  (None, 32, 32, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_57 (Ba  (None, 32, 32, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_58 (Ba  (None, 32, 32, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPooli  (None, 16, 16, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 16, 16, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 16, 16, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_43 (MaxPooli  (None, 8, 8, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 512)               8389120   \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9691525 (36.97 MB)\n",
      "Trainable params: 9688133 (36.96 MB)\n",
      "Non-trainable params: 3392 (13.25 KB)\n",
      "_________________________________________________________________\n",
      "Número de imágenes de entrenamiento:  2506\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\galla\\AppData\\Local\\Temp\\ipykernel_3444\\2950350577.py:56: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size = 32), epochs = 100, validation_data = [X_test, Y_test],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - ETA: 0s - loss: 2.3008 - accuracy: 0.2595\n",
      "Epoch 1: val_accuracy improved from -inf to 0.29187, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 64s 802ms/step - loss: 2.3008 - accuracy: 0.2595 - val_loss: 21.8030 - val_accuracy: 0.2919\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.0849 - accuracy: 0.2801\n",
      "Epoch 2: val_accuracy did not improve from 0.29187\n",
      "78/78 [==============================] - 62s 792ms/step - loss: 2.0849 - accuracy: 0.2801 - val_loss: 3.5629 - val_accuracy: 0.2903\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.8683 - accuracy: 0.3060\n",
      "Epoch 3: val_accuracy improved from 0.29187 to 0.30781, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 62s 801ms/step - loss: 1.8683 - accuracy: 0.3060 - val_loss: 2.1966 - val_accuracy: 0.3078\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.7202 - accuracy: 0.3472\n",
      "Epoch 4: val_accuracy improved from 0.30781 to 0.44179, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 62s 801ms/step - loss: 1.7202 - accuracy: 0.3472 - val_loss: 1.5210 - val_accuracy: 0.4418\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.6738 - accuracy: 0.3581\n",
      "Epoch 5: val_accuracy did not improve from 0.44179\n",
      "78/78 [==============================] - 67s 861ms/step - loss: 1.6738 - accuracy: 0.3581 - val_loss: 1.4649 - val_accuracy: 0.4226\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5523 - accuracy: 0.3965\n",
      "Epoch 6: val_accuracy did not improve from 0.44179\n",
      "78/78 [==============================] - 66s 842ms/step - loss: 1.5523 - accuracy: 0.3965 - val_loss: 1.4232 - val_accuracy: 0.4019\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.5178 - accuracy: 0.4010\n",
      "Epoch 7: val_accuracy did not improve from 0.44179\n",
      "78/78 [==============================] - 64s 816ms/step - loss: 1.5178 - accuracy: 0.4010 - val_loss: 1.4907 - val_accuracy: 0.4370\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.4263 - accuracy: 0.4418\n",
      "Epoch 8: val_accuracy improved from 0.44179 to 0.48963, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 65s 828ms/step - loss: 1.4263 - accuracy: 0.4418 - val_loss: 1.2976 - val_accuracy: 0.4896\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3867 - accuracy: 0.4454\n",
      "Epoch 9: val_accuracy did not improve from 0.48963\n",
      "78/78 [==============================] - 65s 833ms/step - loss: 1.3867 - accuracy: 0.4454 - val_loss: 1.2848 - val_accuracy: 0.4673\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3898 - accuracy: 0.4559\n",
      "Epoch 10: val_accuracy did not improve from 0.48963\n",
      "78/78 [==============================] - 67s 861ms/step - loss: 1.3898 - accuracy: 0.4559 - val_loss: 2.0673 - val_accuracy: 0.2998\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.3045 - accuracy: 0.4834\n",
      "Epoch 11: val_accuracy improved from 0.48963 to 0.59490, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 66s 840ms/step - loss: 1.3045 - accuracy: 0.4834 - val_loss: 1.0233 - val_accuracy: 0.5949\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2497 - accuracy: 0.5044\n",
      "Epoch 12: val_accuracy did not improve from 0.59490\n",
      "78/78 [==============================] - 64s 815ms/step - loss: 1.2497 - accuracy: 0.5044 - val_loss: 1.4503 - val_accuracy: 0.4880\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.2047 - accuracy: 0.5327\n",
      "Epoch 13: val_accuracy did not improve from 0.59490\n",
      "78/78 [==============================] - 64s 815ms/step - loss: 1.2047 - accuracy: 0.5327 - val_loss: 1.0988 - val_accuracy: 0.5853\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.1323 - accuracy: 0.5724\n",
      "Epoch 14: val_accuracy improved from 0.59490 to 0.67624, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 65s 831ms/step - loss: 1.1323 - accuracy: 0.5724 - val_loss: 0.9326 - val_accuracy: 0.6762\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.1216 - accuracy: 0.5521\n",
      "Epoch 15: val_accuracy did not improve from 0.67624\n",
      "78/78 [==============================] - 64s 823ms/step - loss: 1.1216 - accuracy: 0.5521 - val_loss: 1.8945 - val_accuracy: 0.4099\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.0784 - accuracy: 0.5889\n",
      "Epoch 16: val_accuracy did not improve from 0.67624\n",
      "78/78 [==============================] - 63s 810ms/step - loss: 1.0784 - accuracy: 0.5889 - val_loss: 1.0466 - val_accuracy: 0.5789\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 1.0658 - accuracy: 0.5861\n",
      "Epoch 17: val_accuracy did not improve from 0.67624\n",
      "78/78 [==============================] - 64s 817ms/step - loss: 1.0658 - accuracy: 0.5861 - val_loss: 1.2379 - val_accuracy: 0.5630\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.9750 - accuracy: 0.6261\n",
      "Epoch 18: val_accuracy improved from 0.67624 to 0.71770, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 63s 810ms/step - loss: 0.9750 - accuracy: 0.6261 - val_loss: 0.7890 - val_accuracy: 0.7177\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.9735 - accuracy: 0.6403\n",
      "Epoch 19: val_accuracy improved from 0.71770 to 0.72408, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 63s 801ms/step - loss: 0.9735 - accuracy: 0.6403 - val_loss: 0.7044 - val_accuracy: 0.7241\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.9200 - accuracy: 0.6605\n",
      "Epoch 20: val_accuracy did not improve from 0.72408\n",
      "78/78 [==============================] - 61s 788ms/step - loss: 0.9200 - accuracy: 0.6605 - val_loss: 0.8001 - val_accuracy: 0.6938\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.8972 - accuracy: 0.6657\n",
      "Epoch 21: val_accuracy did not improve from 0.72408\n",
      "78/78 [==============================] - 63s 803ms/step - loss: 0.8972 - accuracy: 0.6657 - val_loss: 0.8357 - val_accuracy: 0.7129\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.8519 - accuracy: 0.6787\n",
      "Epoch 22: val_accuracy did not improve from 0.72408\n",
      "78/78 [==============================] - 63s 809ms/step - loss: 0.8519 - accuracy: 0.6787 - val_loss: 1.2116 - val_accuracy: 0.5758\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.8466 - accuracy: 0.6924\n",
      "Epoch 23: val_accuracy improved from 0.72408 to 0.77033, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 64s 817ms/step - loss: 0.8466 - accuracy: 0.6924 - val_loss: 0.6594 - val_accuracy: 0.7703\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.7726 - accuracy: 0.7142\n",
      "Epoch 24: val_accuracy did not improve from 0.77033\n",
      "78/78 [==============================] - 64s 822ms/step - loss: 0.7726 - accuracy: 0.7142 - val_loss: 0.7490 - val_accuracy: 0.7576\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.7580 - accuracy: 0.7264\n",
      "Epoch 25: val_accuracy did not improve from 0.77033\n",
      "78/78 [==============================] - 64s 818ms/step - loss: 0.7580 - accuracy: 0.7264 - val_loss: 0.6442 - val_accuracy: 0.7687\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.7575\n",
      "Epoch 26: val_accuracy improved from 0.77033 to 0.79426, saving model to best_model.hdf5\n",
      "78/78 [==============================] - 64s 824ms/step - loss: 0.7013 - accuracy: 0.7575 - val_loss: 0.5638 - val_accuracy: 0.7943\n",
      "Epoch 27/100\n",
      "48/78 [=================>............] - ETA: 22s - loss: 0.6654 - accuracy: 0.7617"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 56\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:2889\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2878\u001b[0m \n\u001b[0;32m   2879\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2880\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2881\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2883\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2884\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2885\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2886\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2887\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2888\u001b[0m )\n\u001b[1;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2890\u001b[0m     generator,\n\u001b[0;32m   2891\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2892\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2893\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2894\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2895\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2896\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2897\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2898\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2899\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2900\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2901\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2902\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2903\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   2904\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\galla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Script de Construcción del modelo\n",
    "\n",
    "# Importar librerías\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "# Definimos el modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, padding = 'same', activation = 'relu', input_shape =(128, 128, 3), kernel_initializer = 'he_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "print(\"Adri es tonto\")\n",
    "# Mostramos un resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Printear todas las imagenes que usa el modelo (len)\n",
    "print(\"Número de imágenes de entrenamiento: \", len(X_train))\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = model.fit_generator(generador_imagenes.flow(X_train, Y_train, batch_size = 32), epochs = 100, validation_data = [X_test, Y_test],\n",
    "                             steps_per_epoch=len(X_train) // 32, callbacks = [checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21126de3de0b02901272bc1080c53c327c2c88e32dda9e696532b5930d739af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
