{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Aplicación de CNN para reconocimiento de Pokémon y generación de una interfaz explicativa</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Generales.\n",
    "\n",
    "* Asignatura: Inteligencia Artificial en las Organizaciones\n",
    "* Curso: 2023/2024\n",
    "* Profesor: Agapito Ismael Ledezma Espino\n",
    "* Grupo: 85\n",
    "\n",
    "Alumnos: \n",
    "- Jonathan Jiménez Muñoz (100451132@alumnos.uc3m.es)\n",
    "- Marta Palomo Velasco (100451041@alumnos.uc3m.es)\n",
    "- Francisco Antonio Gallardo Fuentes (100451146@alumnos.uc3m.es)\n",
    "- Yago Brotón Gutiérrez (100451322@alumnos.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El objetivo de está práctica va a ser utilizar Redes Neuronales Convolucionales para clasificar imágenes de Pokémon y, una vez identificado dicho Pokémon, construir una pequeña interfaz donde se muestren estadísticas relevantes sobre el mismo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "Cada año salen más y más Pokemon, habiendo actualmente más de 1000 especies diferentes, donde cada especie tiene n movimientos diferentes, 2/3 habilidades diferentes, naturalezas, tipos, etc. <br><br>\n",
    "Es por ello que las personas junior que intentan competir profesionalmente en el juego de Pokemon, se ven abrumadas por la cantidad de información que tienen que procesar, y por ello cada vez hay más y más jugadores masters (+18 años) y menos senior y junior (menos de 18 años), dado que los jugadores que antes eran junior han cambiado de categoría y cada vez hay menos jugadores nuevos que se unan a la comunidad. <br><br>\n",
    "Con esta práctica lo que queremos lograr es que, a partir de una simple imagen de un Pokemon, se muestre información relevante sobre el mismo, como por ejemplo, sus estadísticas base, sus habilidades, sus movimientos, etc, y con ello que los jugadores junior puedan aprender más rápido y tener algo más de motivación para entrar en la comunidad competitiva de Pokemon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo de la práctica\n",
    "\n",
    "Esta práctica va a estar dividida en varias fases:\n",
    "\n",
    "1. Obtención de los datos: En esta fase vamos a obtener aquellas imágenes que vamos a utilizar para entrenar nuestro modelo, y vamos a preprocesarlas para que el modelo pueda entrenar con ellas. Además de obtener los datos usados para la interfaz. <br><br>\n",
    "2. Preprocesado de los datos: Una vez obtengamos todos los datos/imágenes posibles, tendremos que hacer una limpieza (eliminar duplicados, imágenes que no correspondan al Pokemon que queremos, etc), con el objetivo de tener un dataset lo más limpio posible.<br><br>\n",
    "3. División train_test y últimas preparaciones para el modelo: Tras haber preprocesado los datos, el siguiente paso es dividir el dataset en train y test, y realizar las últimas preparaciones para el modelo (generación de más imágenes, etc).<br><br>\n",
    "4. Construcción del modelo: Una vez tenemos los datos preparados, el siguiente paso es construir el modelo. Para ello, vamos a utilizar una red neuronal convolucional CNN, que es un tipo de red neuronal que se utiliza para clasificar imágenes. <br><br>\n",
    "5. Desarrollo de la Interfaz: Finalmente, con el modelo ya entrenado y seleccionado, tendremos que desarrollar una interfaz que muestre aquella información (mejores movimientos, habilidades, estadísticas, etc) que consideremos relevantes para el jugador y que le ayuden a mejorar en el juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtención de los datos\n",
    "\n",
    "Para obtener las imágenes y datos sobre los Pokémon, hemos utilizado diferentes fuentes de información, como por ejemplo:\n",
    "Kaggle: \n",
    "- https://www.kaggle.com/datasets/abcsds/pokemon\n",
    "- https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types\n",
    "- https://www.kaggle.com/datasets/kvpratama/pokemon-images-dataset\n",
    "- https://www.kaggle.com/datasets/arenagrenade/the-complete-pokemon-images-data-set\n",
    "\n",
    "ImagenDex:\n",
    "- https://pokemaster.es/imagendex/\n",
    "\n",
    "Una vez hemos obtenido imágenes de todos los Pokémon, el próximo objetivo es escoger aquellos que más imágenes tenemos y a partir de ahí, obtener más imágenes de los mismos. <br>\n",
    "\n",
    "Para ello decidimos crear un pequeño script que nos permitiera obtener imágenes a partir de Google Imágenes. <br>\n",
    "\n",
    "En dicho script utilizamos las librerías: google_search, para realizar búsquedas en Google Imágenes; requests, para realizar peticiones a las páginas web y BeautifulSoup para parsear el HTML de las páginas web y obtener las imágenes. <br>\n",
    "\n",
    "En caso de requerir de más imágenes, a la hora de generar el modelo utilizaremos algunas técnicas como poner la imagen en modo espejo, rotarla, etc y así obtener hacer que nuestro modelo sea lo más preciso posible. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom googlesearch import search\\n\\n# Función para la descarga de imágenes\\ndef download_all_images(query):\\n    image_links = set()\\n\\n    # Searching for the query in Google Images\\n    search_query = query + \" images\"\\n    for j in search(search_query, num=10, stop=10, pause=2):\\n        if \\'https://encrypted-tbn0.gstatic.com/images\\' in j:\\n            continue\\n        page = requests.get(j)\\n        soup = BeautifulSoup(page.content, \"html.parser\")\\n\\n        # Extracting image links\\n        for raw_img in soup.find_all(\"img\"):\\n            link = raw_img.get(\"src\")\\n            if link and link.startswith(\"http\"):\\n                image_links.add(link)\\n\\n    # Create a directory for downloaded images\\n    if not os.path.exists(query):\\n        os.makedirs(query)\\n\\n    # Download the images\\n    for i, link in enumerate(image_links):\\n        try:\\n            response = requests.get(link)\\n            file = open(os.path.join(query, f\"{query}_{i+1}.jpg\"), \"wb\")\\n            file.write(response.content)\\n            file.close()\\n        except Exception as e:\\n            print(f\"Error: {e}\")\\n            continue\\n    \\n    print(f\"Downloaded {len(image_links)} images for {query}.\")\\n    return\\n\\n\\nquery = \"squirtle in game\"  # Introducir el término aquí.\\n\\ndownload_all_images(query)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la búsqueda de imágenes en Google imágenes (descomentar si se quiere probar)\n",
    "\n",
    "# Importamos las librerías necesarias\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "\n",
    "# Función para la descarga de imágenes\n",
    "def download_all_images(query):\n",
    "    image_links = set()\n",
    "\n",
    "    # Searching for the query in Google Images\n",
    "    search_query = query + \" images\"\n",
    "    for j in search(search_query, num=10, stop=10, pause=2):\n",
    "        if 'https://encrypted-tbn0.gstatic.com/images' in j:\n",
    "            continue\n",
    "        page = requests.get(j)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # Extracting image links\n",
    "        for raw_img in soup.find_all(\"img\"):\n",
    "            link = raw_img.get(\"src\")\n",
    "            if link and link.startswith(\"http\"):\n",
    "                image_links.add(link)\n",
    "\n",
    "    # Create a directory for downloaded images\n",
    "    if not os.path.exists(query):\n",
    "        os.makedirs(query)\n",
    "\n",
    "    # Download the images\n",
    "    for i, link in enumerate(image_links):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            file = open(os.path.join(query, f\"{query}_{i+1}.jpg\"), \"wb\")\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Downloaded {len(image_links)} images for {query}.\")\n",
    "    return\n",
    "\n",
    "\n",
    "query = \"squirtle in game\"  # Introducir el término aquí.\n",
    "\n",
    "download_all_images(query)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargadas las imágenes, vamos a escribir un nombre en cada imagen igual, para que después sea más sencillo el preprocesado de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Importar librerías\\nimport os\\n\\npokemon = 'Squirtle' # Cambiar para cada nombre de pokemon\\npath = 'pokemon/'+pokemon\\n\\n# Recorremos las imágenes de la carpeta y vamos cambiando el nombre\\ni = 0\\nfor filename in os.listdir(path):\\n    os.rename(path+'/'+filename, path+'/'+pokemon+'__'+str(i)+'.png')\\n    i += 1\\n    \\nprint('Nombres cambiados')\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para cambiar el nombre de las imágenes descargadas (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "# Importar librerías\n",
    "import os\n",
    "\n",
    "pokemon = 'Squirtle' # Cambiar para cada nombre de pokemon\n",
    "path = 'pokemon/'+pokemon\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el nombre\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    os.rename(path+'/'+filename, path+'/'+pokemon+'__'+str(i)+'.png')\n",
    "    i += 1\n",
    "    \n",
    "print('Nombres cambiados')\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo esto, obtuvimos muchas imágenes para cada Pokémon. Sin embargo, decidimos quedarnos con los Pokémon que más imágenes tenían, debido a que consideramos que es mejor obtener un buen modelo que funcione para una cantidad reducida de especies, que crear un modelo que funcione para muchas especies pero no sea demasiado preciso. <br>\n",
    "\n",
    "Por ello, los Pokémon que decidimos utilizar fueron los siguientes: Pikachu, Charmander, Charizard, Caterpie, Magikarp, Ratata, Geodude, Machop, Squirtle, Bulbasaur, Mew, Dragonite, Meowth, Lapras, Snorlax, Greninja, Rayquaza, Lucario, MrMime y Gengar (20 especies). <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesado de los datos\n",
    "Debido a que utilizamos diferentes filtros en la búsqueda de imágenes para obtener las máximas posibles, nos salieron muchas imágenes duplicadas o que no se correspondían con el Pokémon que queríamos. <br>\n",
    "\n",
    "Es por ello que debíamos hacer un preprocesado de los datos, para quedarnos con el conjunto de imágenes de cada Pokémon lo más limpio y homogéneo posible. <br>\n",
    "\n",
    "Esto conyevó varias tareas: \n",
    "- Eliminación de imágenes no correspondientes al Pokémon que queríamos. Para ello simplemente una vez descargadas todas las imágenes, nos metíamos en la carpeta e íbamos eliminando una a una las imágenes que no pertenecían a dicho Pokémon.\n",
    "- Eliminación de imágenes duplicadas. En este punto nos dimos cuenta de que hacerlo a mano iba a ser prácticamente imposible, por ello decidimos crear un pequeño script que comparase las imágenes entre sí y eliminase aquellas que fueran iguales, usando la libería Pilow.\n",
    "- Normalizar, redimensionar y convertir las imágenes a escala de grises. Para ello, utilizaremos keras.preprocessing.image, que nos permite realizar estas tareas de forma muy sencilla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Scripts usados para el preprocesdo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nimport os\\n\\n# Ruta de la carpeta con las imágenes\\ncarpeta = \\'pokemon/Meowth\\'\\n\\n# Diccionario para almacenar las imágenes idénticas\\nimagenes_ident = {}\\n\\n# Recorriendo la carpeta y comparando cada imagen con las demás\\nfor nombre_imagen1 in os.listdir(carpeta):\\n    ruta_imagen1 = os.path.join(carpeta, nombre_imagen1)\\n    if os.path.isfile(ruta_imagen1):\\n        imagen1 = Image.open(ruta_imagen1)\\n        for nombre_imagen2 in os.listdir(carpeta):\\n            ruta_imagen2 = os.path.join(carpeta, nombre_imagen2)\\n            if os.path.isfile(ruta_imagen2) and ruta_imagen1 != ruta_imagen2:\\n                imagen2 = Image.open(ruta_imagen2)\\n                if imagen1.size == imagen2.size and list(imagen1.getdata()) == list(imagen2.getdata()):\\n                    if nombre_imagen1 not in imagenes_ident:\\n                        imagenes_ident[nombre_imagen1] = [nombre_imagen2]\\n                    else:\\n                        imagenes_ident[nombre_imagen1].append(nombre_imagen2)\\n\\n# Mostrar las imágenes idénticas encontradas\\nfor imagen, imagenes_iguales in imagenes_ident.items():\\n    print(f\"La imagen {imagen} es idéntica a: {\\', \\'.join(imagenes_iguales)}\")\\n    \\n# Sacamos la imagen duplicada de la carpeta y la movemos a otra carpeta, dejando solamente una copia de la imagen\\nfor imagen, imagenes_iguales in imagenes_ident.items():\\n    # si no existe la carpeta la creamos\\n    if not os.path.exists(\\'pokemon/duplicadas\\'):\\n        os.makedirs(\\'pokemon/duplicadas\\')\\n    # movemos la imagen a la carpeta duplicadas\\n    os.rename(carpeta + \\'/\\' + imagen, \\'pokemon/duplicadas/\\' + imagen)\\n    # borramos las imagenes duplicadas\\n    for imagen_duplicada in imagenes_iguales:\\n        os.remove(carpeta + \\'/\\' + imagen_duplicada)\\n     \\n    \\n# Si no hay imágenes idénticas, mostrar un mensaje\\nif len(imagenes_ident) == 0:\\n    print(\"No hay imágenes idénticas.\")\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script de eliminación de imágenes duplicadas (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Ruta de la carpeta con las imágenes\n",
    "carpeta = 'pokemon/Meowth'\n",
    "\n",
    "# Diccionario para almacenar las imágenes idénticas\n",
    "imagenes_ident = {}\n",
    "\n",
    "# Recorriendo la carpeta y comparando cada imagen con las demás\n",
    "for nombre_imagen1 in os.listdir(carpeta):\n",
    "    ruta_imagen1 = os.path.join(carpeta, nombre_imagen1)\n",
    "    if os.path.isfile(ruta_imagen1):\n",
    "        imagen1 = Image.open(ruta_imagen1)\n",
    "        for nombre_imagen2 in os.listdir(carpeta):\n",
    "            ruta_imagen2 = os.path.join(carpeta, nombre_imagen2)\n",
    "            if os.path.isfile(ruta_imagen2) and ruta_imagen1 != ruta_imagen2:\n",
    "                imagen2 = Image.open(ruta_imagen2)\n",
    "                if imagen1.size == imagen2.size and list(imagen1.getdata()) == list(imagen2.getdata()):\n",
    "                    if nombre_imagen1 not in imagenes_ident:\n",
    "                        imagenes_ident[nombre_imagen1] = [nombre_imagen2]\n",
    "                    else:\n",
    "                        imagenes_ident[nombre_imagen1].append(nombre_imagen2)\n",
    "\n",
    "# Mostrar las imágenes idénticas encontradas\n",
    "for imagen, imagenes_iguales in imagenes_ident.items():\n",
    "    print(f\"La imagen {imagen} es idéntica a: {', '.join(imagenes_iguales)}\")\n",
    "    \n",
    "# Sacamos la imagen duplicada de la carpeta y la movemos a otra carpeta, dejando solamente una copia de la imagen\n",
    "for imagen, imagenes_iguales in imagenes_ident.items():\n",
    "    # si no existe la carpeta la creamos\n",
    "    if not os.path.exists('pokemon/duplicadas'):\n",
    "        os.makedirs('pokemon/duplicadas')\n",
    "    # movemos la imagen a la carpeta duplicadas\n",
    "    os.rename(carpeta + '/' + imagen, 'pokemon/duplicadas/' + imagen)\n",
    "    # borramos las imagenes duplicadas\n",
    "    for imagen_duplicada in imagenes_iguales:\n",
    "        os.remove(carpeta + '/' + imagen_duplicada)\n",
    "     \n",
    "    \n",
    "# Si no hay imágenes idénticas, mostrar un mensaje\n",
    "if len(imagenes_ident) == 0:\n",
    "    print(\"No hay imágenes idénticas.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Importar librerías\\nimport os\\nfrom PIL import Image\\n\\n# Seleccionamos la carpeta con las imágenes a normalizar\\npokemon = \\'Pikachu\\' # Cambiar para cada nombre de pokemon\\npath = \\'pokemon/\\'+pokemon\\n\\n# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\\ni = 0\\nfor filename in os.listdir(path):\\n    # Normalizar\\n    img = Image.open(path+\\'/\\'+filename)\\n    img = img.resize((128,128))\\n    img.save(path+\\'/\\'+pokemon+\\'_\\'+str(i)+\\'.png\\')\\n    i += 1\\n    \\nprint(\\'Imágenes normalizadas\\')\\n\\n# Eliminar todos los jpg para evitar duplicados en varios formatos\\n\\nfor filename in os.listdir(path):\\n    if filename.endswith(\".jpg\"):\\n        os.remove(path+\\'/\\'+filename)\\n        \\nprint(\\'Imágenes jpg eliminadas\\')\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la normalización (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Seleccionamos la carpeta con las imágenes a normalizar\n",
    "pokemon = 'Pikachu' # Cambiar para cada nombre de pokemon\n",
    "path = 'pokemon/'+pokemon\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    # Normalizar\n",
    "    img = Image.open(path+'/'+filename)\n",
    "    img = img.resize((128,128))\n",
    "    img.save(path+'/'+pokemon+'_'+str(i)+'.png')\n",
    "    i += 1\n",
    "    \n",
    "print('Imágenes normalizadas')\n",
    "\n",
    "# Eliminar todos los jpg para evitar duplicados en varios formatos\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        os.remove(path+'/'+filename)\n",
    "        \n",
    "print('Imágenes jpg eliminadas')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Importar librerías\\nimport os\\nimport cv2\\nimport numpy as np\\n\\n# Seleccionamos la carpeta con las imágenes a cambiar\\npokemon = 'MrMime' # Cambiar para cada nombre de pokemon\\npath = 'pokemon/'+pokemon\\n\\n# Recorremos las imágenes de la carpeta y vamos aplicando el filtro\\ni = 0\\nfor filename in os.listdir(path):\\n    # Normalizar\\n    img = cv2.imread(path+'/'+filename)\\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    cv2.imwrite(path+'/'+pokemon+'_'+str(i)+'.png',gray)\\n    i += 1\\n    \\nprint('Imágenes en escala de grises')\\n\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la conversión a escala de grises (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Seleccionamos la carpeta con las imágenes a cambiar\n",
    "pokemon = 'MrMime' # Cambiar para cada nombre de pokemon\n",
    "path = 'pokemon/'+pokemon\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos aplicando el filtro\n",
    "i = 0\n",
    "for filename in os.listdir(path):\n",
    "    # Normalizar\n",
    "    img = cv2.imread(path+'/'+filename)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(path+'/'+pokemon+'_'+str(i)+'.png',gray)\n",
    "    i += 1\n",
    "    \n",
    "print('Imágenes en escala de grises')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos anteriormente, a pesar de haber realizado una búsqueda masiva de imágenes, creemos que con apenas 400-1000 imágenes de cada especie no son suficientes, es por ello que vamos a utilizar técnicas como la rotación o volteo y así obtener, como mínimo, 2000 imágenes de cada especie. Para ello, podemos hacerlo de dos maneras:\n",
    "1. Realizarlo directamente sobre el conjunto de imágenes.\n",
    "2. Realizar la división del conjunto de imágenes en train y test y aplicar las técnicas de rotación y volteo sobre el conjunto de train.\n",
    "\n",
    "Cualquiera de las opciones debería ser correcta, pero hemos decidido realizar la segunda opción, ya que así podemos usar directamente el conjunto de train a la hora de crear el modelo y no tendríamos que hacer una división posteriormente, es decir, nos ahorra un paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Generamos 3000 imágenes de cada especie a partir de las 400-1000 que teníamos.\\n\\n# Importamos librerías\\nfrom keras.preprocessing.image import ImageDataGenerator\\nimport random\\n\\n# Generador de imágenes\\ngenerador_imagenes = ImageDataGenerator(rotation_range=10, # Rotación aleatoria de la imagen\\n                                        width_shift_range=0.1, # Desplazamiento horizontal aleatorio\\n                                        height_shift_range=0.1, # Desplazamiento vertical aleatorio\\n                                        zoom_range=0.1, # Zoom aleatorio\\n                                        horizontal_flip=True) # Volteo horizontal\\n\\n# Generamos 50 imágenes de cada imagen que tenemos y se guardan en la carpeta de la clase correspondiente\\nfor elemento in clases:\\n    # Recorremos las imágenes de cada clase\\n    for filename in os.listdir(path+'/'+elemento):\\n        # Añadimos la imagen a la lista X\\n        img = cv2.imread(path+'/'+elemento+'/'+filename)\\n        # Revisar que todas las imágenes tienen el mismo tamaño\\n        img = cv2.resize(img, (128,128))\\n        # Cambiamos la dimensión de la imagen\\n        img = img.reshape([-1, 128, 128, 3])\\n        # Generamos las imágenes\\n        generador_imagenes.fit(img)\\n        imagenes_generadas = generador_imagenes.flow(img, batch_size=50, save_to_dir=path+'/'+elemento, save_prefix='aug', save_format='png')\\n        # Guardamos las imágenes\\n        for i in range(10):\\n            imagenes_generadas.next()\\n            \\nprint('Imágenes generadas')\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generamos más imágenes (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "# Generamos 3000 imágenes de cada especie a partir de las 400-1000 que teníamos.\n",
    "\n",
    "# Importamos librerías\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "\n",
    "# Generador de imágenes\n",
    "generador_imagenes = ImageDataGenerator(rotation_range=10, # Rotación aleatoria de la imagen\n",
    "                                        width_shift_range=0.1, # Desplazamiento horizontal aleatorio\n",
    "                                        height_shift_range=0.1, # Desplazamiento vertical aleatorio\n",
    "                                        zoom_range=0.1, # Zoom aleatorio\n",
    "                                        horizontal_flip=True) # Volteo horizontal\n",
    "\n",
    "# Generamos 50 imágenes de cada imagen que tenemos y se guardan en la carpeta de la clase correspondiente\n",
    "for elemento in clases:\n",
    "    # Recorremos las imágenes de cada clase\n",
    "    for filename in os.listdir(path+'/'+elemento):\n",
    "        # Añadimos la imagen a la lista X\n",
    "        img = cv2.imread(path+'/'+elemento+'/'+filename)\n",
    "        # Revisar que todas las imágenes tienen el mismo tamaño\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        # Cambiamos la dimensión de la imagen\n",
    "        img = img.reshape([-1, 128, 128, 3])\n",
    "        # Generamos las imágenes\n",
    "        generador_imagenes.fit(img)\n",
    "        imagenes_generadas = generador_imagenes.flow(img, batch_size=50, save_to_dir=path+'/'+elemento, save_prefix='aug', save_format='png')\n",
    "        # Guardamos las imágenes\n",
    "        for i in range(10):\n",
    "            imagenes_generadas.next()\n",
    "            \n",
    "print('Imágenes generadas')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, conseguimos tener un dataset lo más limpio posible y preparado para el siguiente punto: realización del modelo y entrenamiento <directorio ./Pokemon>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar con la creación del modelo, vamos a visualizar a través de gráficas las diferentes clases que tenemos en nuestro dataset, con el objetivo de ver el número de imágenes que tenemos para cada clase, si hay desbalanceo o no. Recordamos que como tenemos 20 pokémon, el número de clases será 20. <br>\n",
    "Para la realización de este modelo, nos hemos basado en páginas de referencia tales como: stackoverflow, tensorflow y keras; en los cuales hemos usado el código que nos proporcionan, realizando las modificaciones necesarias para adaptarlo a nuestro problema. <br>\n",
    "- TensorFlow: https://www.tensorflow.org/tutorials/images/classification\n",
    "- Keras: https://keras.io/examples/vision/image_classification_from_scratch/\n",
    "- Stackoverflow: usado para resolver dudas puntuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de clases:  20\n",
      "Número de imágenes por clase:  {'Greninja': 308, 'Meowth': 576, 'MrMime': 293, 'Machop': 184, 'Dragonite': 461, 'Squirtle': 521, 'Geodude': 218, 'Caterpie': 173, 'Charizard': 424, 'Rattata': 49, 'Bulbasaur': 313, 'Pikachu': 853, 'Lapras': 417, 'Magikarp': 373, 'Charmander': 680, 'Snorlax': 456, 'Gengar': 334, 'Eevee': 340, 'Rayquaza': 433, 'Jigglypuff': 503}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Script para pre-visualizar las clases y el número de imágenes por clase (descomentar si se quiere probar)\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionamos la carpeta con las diferentes clases\n",
    "path = 'pokemon'\n",
    "clases = os.listdir(path) # Lista con las clases\n",
    "print(\"Número total de clases: \", len(clases))\n",
    "\n",
    "# Creamos un diccionario para almacenar el número de imágenes por clase\n",
    "num_imagenes = {}\n",
    "for elemento in clases:\n",
    "    num_imagenes[elemento] = len(os.listdir(path+'/'+elemento))\n",
    "    \n",
    "print(\"Número de imágenes por clase: \", num_imagenes)\n",
    "\n",
    "# cogemos solo las 5 clases con más imágenes\n",
    "#num_imagenes = {k: v for k, v in sorted(num_imagenes.items(), key=lambda item: item[1], reverse=True)[:5]}\n",
    "#clases = list(num_imagenes.keys())\n",
    "#print(\"Número de clases: \", len(clases))\n",
    "#print(\"Número de imágenes por clase: \", num_imagenes)\n",
    "\n",
    "# Dibujamos la gráfica\n",
    "plt.bar(range(len(num_imagenes)), list(num_imagenes.values()), align='center')\n",
    "plt.xticks(range(len(num_imagenes)), list(num_imagenes.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes:  7909\n",
      "Número de etiquetas:  7909\n",
      "Forma de X_train:  (6327, 128, 128, 3)\n",
      "Forma de X_test:  (1582, 128, 128, 3)\n",
      "Forma de Y_train:  (6327,)\n",
      "Forma de Y_test:  (1582,)\n"
     ]
    }
   ],
   "source": [
    "# Definimos el Conjunto X e Y, donde X son las imágenes y Y las clases\n",
    "\n",
    "# Importar librerías\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "X = [] # Lista con las imágenes\n",
    "Y = [] # Lista de etiquetas\n",
    "\n",
    "# Recorremos las clases\n",
    "for elemento in clases:\n",
    "    # Recorremos las imágenes de cada clase\n",
    "    for filename in os.listdir(path+'/'+elemento):\n",
    "        # Añadimos la imagen a la lista X\n",
    "        img = cv2.imread(path+'/'+elemento+'/'+filename)\n",
    "        # Revisar que todas las imágenes tienen el mismo tamaño\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        X.append(img)\n",
    "        Y.append(elemento)\n",
    "        \n",
    "# Imprimimos la longitud de las listas\n",
    "print(\"Número de imágenes: \", len(X))\n",
    "print(\"Número de etiquetas: \", len(Y))\n",
    "\n",
    "# Convertimos las listas a arrays de numpy\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    " \n",
    "# División del conjunto de datos en entrenamiento y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=22, stratify=Y, shuffle=True)\n",
    "print(\"Forma de X_train: \", X_train.shape)\n",
    "print(\"Forma de X_test: \", X_test.shape)\n",
    "print(\"Forma de Y_train: \", Y_train.shape)\n",
    "print(\"Forma de Y_test: \", Y_test.shape)\n",
    "\n",
    "# Codificación one hot de las etiquetas\n",
    "encoder = LabelEncoder() \n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "Y_test = encoder.transform(Y_test)\n",
    "Y_train = to_categorical(Y_train, num_classes=20)\n",
    "Y_test = to_categorical(Y_test, num_classes=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos antes, vamos a generar más imágenes a aprtir de técnicas como la rotación, modo espejo, zoom, etc. Para ello, vamos a utilizar la librería ImageDataGenerator de keras, que nos permite realizar estas técnicas de forma muy sencilla. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\\ngenerador_imagenes = ImageDataGenerator(rotation_range = 45, # Graus de rotación\\n                            zoom_range = 0.2, # Zoom a aplicar\\n                            horizontal_flip = True, # Volteo horizontal\\n                            width_shift_range = 0.15, # Desplazamiento horizontal\\n                            height_shift_range = 0.15, # Desplazamiento vertical\\n                            shear_range = 0.2) # Cizallamiento\\n\\ngenerador_imagenes.fit(X_train)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "generador_imagenes = ImageDataGenerator(rotation_range = 45, # Graus de rotación\n",
    "                            zoom_range = 0.2, # Zoom a aplicar\n",
    "                            horizontal_flip = True, # Volteo horizontal\n",
    "                            width_shift_range = 0.15, # Desplazamiento horizontal\n",
    "                            height_shift_range = 0.15, # Desplazamiento vertical\n",
    "                            shear_range = 0.2) # Cizallamiento\n",
    "\n",
    "generador_imagenes.fit(X_train)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construcción del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos ya el conjunto de datos dividido en train and test, todas las imágenes necesarias generadas y el dataset limpio y preparado, vamos a proceder a la creación del modelo CNN. <br>\n",
    "Para ello seguiremos utilizando la librerçia Keras, la cual nos parece muy útil para realizar esta construcción y, además, contiene muchos ejemplos reales y en los cuales podemos orientarnos a la hora de realizar nuestro modelo. <br>\n",
    "A la hora de su realización, podemos basarnos en varios tipos de enfoque: <br> \n",
    "1. Secuencial: se trata de una pila de capas de redes neuronales, en la cual cada capa tiene una entrada y una salida: https://keras.io/guides/sequential_model/ <br>\n",
    "2. Funcional: se trata de un modelo más complejo, en el cual podemos tener varias entradas y salidas, y además podemos tener capas compartidas: https://keras.io/guides/functional_api/<br>\n",
    "\n",
    "Nosotros hemos decidido utilizar el modelo secuencial, ya que consideramos que es más sencillo y más adecuado para nuestro problema dado que como lo que queremos hacer es que, dada una imagen se genere una interfaz que nos diga que Pokémon es, solo necesitamos de una entrada y una salida. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adri es tonto\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 128, 128, 32)      128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 64, 64, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 64, 64, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 64, 64, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 32, 32, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 32, 32, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 32, 32, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 16, 16, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 16, 16, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 16, 16, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 8, 8, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               8389120   \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 256)               1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 20)                5140      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9695380 (36.98 MB)\n",
      "Trainable params: 9691988 (36.97 MB)\n",
      "Non-trainable params: 3392 (13.25 KB)\n",
      "_________________________________________________________________\n",
      "Número de imágenes de entrenamiento:  57747\n",
      "Epoch 1/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 2.3617 - accuracy: 0.3272\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56958, saving model to modelo_up.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmjon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804/1804 [==============================] - 1263s 689ms/step - loss: 2.3617 - accuracy: 0.3272 - val_loss: 1.4334 - val_accuracy: 0.5696\n",
      "Epoch 2/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 1.1978 - accuracy: 0.6441\n",
      "Epoch 2: val_accuracy improved from 0.56958 to 0.78056, saving model to modelo_up.hdf5\n",
      "1804/1804 [==============================] - 1280s 710ms/step - loss: 1.1978 - accuracy: 0.6441 - val_loss: 0.7197 - val_accuracy: 0.7806\n",
      "Epoch 3/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.7842\n",
      "Epoch 3: val_accuracy improved from 0.78056 to 0.83376, saving model to modelo_up.hdf5\n",
      "1804/1804 [==============================] - 1236s 685ms/step - loss: 0.7191 - accuracy: 0.7842 - val_loss: 0.5571 - val_accuracy: 0.8338\n",
      "Epoch 4/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.8532\n",
      "Epoch 4: val_accuracy improved from 0.83376 to 0.88058, saving model to modelo_up.hdf5\n",
      "1804/1804 [==============================] - 1235s 685ms/step - loss: 0.4814 - accuracy: 0.8532 - val_loss: 0.3864 - val_accuracy: 0.8806\n",
      "Epoch 5/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.8991\n",
      "Epoch 5: val_accuracy improved from 0.88058 to 0.93385, saving model to modelo_up.hdf5\n",
      "1804/1804 [==============================] - 1238s 686ms/step - loss: 0.3247 - accuracy: 0.8991 - val_loss: 0.2120 - val_accuracy: 0.9339\n",
      "Epoch 6/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9202\n",
      "Epoch 6: val_accuracy improved from 0.93385 to 0.94985, saving model to modelo_up.hdf5\n",
      "1804/1804 [==============================] - 1235s 685ms/step - loss: 0.2560 - accuracy: 0.9202 - val_loss: 0.1611 - val_accuracy: 0.9499\n",
      "Epoch 7/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9354\n",
      "Epoch 7: val_accuracy improved from 0.94985 to 0.96516, saving model to modelo_up.hdf5\n",
      "1804/1804 [==============================] - 1237s 686ms/step - loss: 0.2063 - accuracy: 0.9354 - val_loss: 0.1159 - val_accuracy: 0.9652\n",
      "Epoch 8/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9491\n",
      "Epoch 8: val_accuracy did not improve from 0.96516\n",
      "1804/1804 [==============================] - 1318s 731ms/step - loss: 0.1606 - accuracy: 0.9491 - val_loss: 0.1238 - val_accuracy: 0.9636\n",
      "Epoch 9/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9547\n",
      "Epoch 9: val_accuracy did not improve from 0.96516\n",
      "1804/1804 [==============================] - 1439s 797ms/step - loss: 0.1431 - accuracy: 0.9547 - val_loss: 0.1166 - val_accuracy: 0.9650\n",
      "Epoch 10/20\n",
      "1804/1804 [==============================] - ETA: 0s - loss: 0.1266 - accuracy: 0.9595\n",
      "Epoch 10: val_accuracy did not improve from 0.96516\n",
      "1804/1804 [==============================] - 1352s 749ms/step - loss: 0.1266 - accuracy: 0.9595 - val_loss: 0.2137 - val_accuracy: 0.9363\n",
      "Epoch 11/20\n",
      " 139/1804 [=>............................] - ETA: 21:55 - loss: 0.1402 - accuracy: 0.9549"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Visual_Projects\\IA-Practica-Final-1\\Desarrollo_Practica.ipynb Cell 27\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Visual_Projects/IA-Practica-Final-1/Desarrollo_Practica.ipynb#X35sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mmodelo_up.hdf5\u001b[39m\u001b[39m'\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, monitor \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, save_best_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Visual_Projects/IA-Practica-Final-1/Desarrollo_Practica.ipynb#X35sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Visual_Projects/IA-Practica-Final-1/Desarrollo_Practica.ipynb#X35sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, Y_train, batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m, validation_data \u001b[39m=\u001b[39;49m [X_test, Y_test],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Visual_Projects/IA-Practica-Final-1/Desarrollo_Practica.ipynb#X35sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                              steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(X_train) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m32\u001b[39;49m, callbacks \u001b[39m=\u001b[39;49m [checkpoint])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Visual_Projects/IA-Practica-Final-1/Desarrollo_Practica.ipynb#X35sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mmodelo_up_20.hdf5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Script de Construcción del modelo\n",
    "\n",
    "# Importar librerías\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "# Definimos el modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, padding = 'same', activation = 'relu', input_shape =(128, 128, 3), kernel_initializer = 'he_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation = 'softmax'))\n",
    "print(\"Adri es tonto\")\n",
    "# Mostramos un resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Printear todas las imagenes que usa el modelo (len)\n",
    "print(\"Número de imágenes de entrenamiento: \", len(X_train))\n",
    "\n",
    "checkpoint = ModelCheckpoint('modelo_up.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 20, validation_data = [X_test, Y_test],\n",
    "                             steps_per_epoch=len(X_train) // 32, callbacks = [checkpoint])\n",
    "\n",
    "model.save('modelo_up_20.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 282ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "Rayquaza\n",
      "1/1 [==============================] - 0s 272ms/step\n",
      "Rayquaza\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path_input = 'input'\n",
    "path_output = 'output'\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\n",
    "i = 0\n",
    "for filename in os.listdir(path_input):\n",
    "    # Normalizar\n",
    "    img = Image.open(path_input+'/'+filename)\n",
    "    img = img.resize((128,128))\n",
    "    img.save(path_output+'/'+'prueba'+'_'+str(i)+'.png')\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(path_output):\n",
    "    # Normalizar\n",
    "    img = cv2.imread(path_output+'/'+filename)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(path_output+'/'+'prueba'+'_'+str(i)+'.png',gray)\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(path_output):\n",
    "\timg = image.load_img('output/' + filename, target_size=(128, 128))\n",
    "\timg_array = image.img_to_array(img)\n",
    "\timg_array = np.expand_dims(img_array, axis=0)\n",
    "\tmodelo = load_model('modelo_up.hdf5')\n",
    "\tprediccion = modelo.predict(img_array)\n",
    "\tindice_prediccion = np.argmax(prediccion)\n",
    "\tclases = {0: 'Bulbasaur', 1: 'Caterpie', 2: 'Charizard', 3: 'Charmander', 4: 'Dragonite', 5: 'Eevee',\n",
    "        \t6: 'Gengar', 7: 'Geodude', 8: 'Greninja', 9: 'Jigglypuff', 10: 'Lapras', 11: 'Machop', 12: 'Magikarp',\n",
    "            13: 'Meowth', 14: \"MrMime\", 15: 'Pikachu', 16: 'Rattata', 17: 'Rayquaza', 18: 'Snorlax', 19: 'Squirtle'}\n",
    "\tpokemon_predicho = clases[indice_prediccion]\n",
    "\tprint(pokemon_predicho)\n",
    "\ti += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21126de3de0b02901272bc1080c53c327c2c88e32dda9e696532b5930d739af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
