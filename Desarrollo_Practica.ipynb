{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Aplicación de CNN para reconocimiento de Pokémon y generación de una interfaz explicativa</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Generales.\n",
    "\n",
    "* Asignatura: Inteligencia Artificial en las Organizaciones\n",
    "* Curso: 2023/2024\n",
    "* Profesor: Agapito Ismael Ledezma Espino\n",
    "* Grupo: 85\n",
    "\n",
    "Alumnos: \n",
    "- Jonathan Jiménez Muñoz (100451132@alumnos.uc3m.es)\n",
    "- Marta Palomo Velasco (100451041@alumnos.uc3m.es)\n",
    "- Francisco Antonio Gallardo Fuentes (100451146@alumnos.uc3m.es)\n",
    "- Yago Brotón Gutiérrez (100451322@alumnos.uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El objetivo de está práctica va a ser utilizar Redes Neuronales Convolucionales para clasificar imágenes de Pokémon y, una vez identificado dicho Pokémon, construir una pequeña interfaz donde se muestren estadísticas relevantes sobre el mismo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "Cada año salen más y más Pokemon, habiendo actualmente más de 1000 especies diferentes, donde cada especie tiene n movimientos diferentes, 2/3 habilidades diferentes, naturalezas, tipos, etc. <br><br>\n",
    "Es por ello que las personas junior que intentan competir profesionalmente en el juego de Pokemon, se ven abrumadas por la cantidad de información que tienen que procesar, y por ello cada vez hay más y más jugadores masters (+18 años) y menos senior y junior (menos de 18 años), dado que los jugadores que antes eran junior han cambiado de categoría y cada vez hay menos jugadores nuevos que se unan a la comunidad. <br><br>\n",
    "Con esta práctica lo que queremos lograr es que, a partir de una simple imagen de un Pokemon, se muestre información relevante sobre el mismo, como por ejemplo, sus estadísticas base, sus habilidades, sus movimientos, etc, y con ello que los jugadores junior puedan aprender más rápido y tener algo más de motivación para entrar en la comunidad competitiva de Pokemon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo de la práctica\n",
    "\n",
    "Esta práctica va a estar dividida en varias fases:\n",
    "\n",
    "1. Obtención de los datos: En esta fase vamos a obtener aquellas imágenes que vamos a utilizar para entrenar nuestro modelo, y vamos a preprocesarlas para que el modelo pueda entrenar con ellas. Además de obtener los datos usados para la interfaz. <br><br>\n",
    "2. Preprocesado de los datos: Una vez obtengamos todos los datos/imágenes posibles, tendremos que hacer una limpieza (eliminar duplicados, imágenes que no correspondan al Pokemon que queremos, etc), con el objetivo de tener un dataset lo más limpio posible.<br><br>\n",
    "3. División train_test y últimas preparaciones para el modelo: Tras haber preprocesado los datos, el siguiente paso es dividir el dataset en train y test, y realizar las últimas preparaciones para el modelo (generación de más imágenes, etc).<br><br>\n",
    "4. Construcción del modelo: Una vez tenemos los datos preparados, el siguiente paso es construir el modelo. Para ello, vamos a utilizar una red neuronal convolucional CNN, que es un tipo de red neuronal que se utiliza para clasificar imágenes. <br><br>\n",
    "5. Desarrollo de la Interfaz: Finalmente, con el modelo ya entrenado y seleccionado, tendremos que desarrollar una interfaz que muestre aquella información (mejores movimientos, habilidades, estadísticas, etc) que consideremos relevantes para el jugador y que le ayuden a mejorar en el juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtención de los datos\n",
    "\n",
    "Para obtener las imágenes y datos sobre los Pokémon, hemos utilizado diferentes fuentes de información, como por ejemplo:\n",
    "Kaggle: \n",
    "- https://www.kaggle.com/datasets/abcsds/pokemon\n",
    "- https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types\n",
    "- https://www.kaggle.com/datasets/kvpratama/pokemon-images-dataset\n",
    "- https://www.kaggle.com/datasets/arenagrenade/the-complete-pokemon-images-data-set\n",
    "\n",
    "ImagenDex:\n",
    "- https://pokemaster.es/imagendex/\n",
    "\n",
    "Una vez hemos obtenido imágenes de todos los Pokémon, el próximo objetivo es escoger aquellos que más imágenes tenemos y a partir de ahí, obtener más imágenes de los mismos. <br>\n",
    "\n",
    "Para ello decidimos crear un pequeño script que nos permitiera obtener imágenes a partir de Google Imágenes. <br>\n",
    "\n",
    "En dicho script utilizamos las librerías: google_search, para realizar búsquedas en Google Imágenes; requests, para realizar peticiones a las páginas web y BeautifulSoup para parsear el HTML de las páginas web y obtener las imágenes. <br>\n",
    "\n",
    "En caso de requerir de más imágenes, a la hora de generar el modelo utilizaremos algunas técnicas como poner la imagen en modo espejo, rotarla, etc y así obtener hacer que nuestro modelo sea lo más preciso posible. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom googlesearch import search\\n\\n# Función para la descarga de imágenes\\ndef download_all_images(query):\\n    image_links = set()\\n\\n    # Searching for the query in Google Images\\n    search_query = query + \" images\"\\n    for j in search(search_query, num=10, stop=10, pause=2):\\n        if \\'https://encrypted-tbn0.gstatic.com/images\\' in j:\\n            continue\\n        page = requests.get(j)\\n        soup = BeautifulSoup(page.content, \"html.parser\")\\n\\n        # Extracting image links\\n        for raw_img in soup.find_all(\"img\"):\\n            link = raw_img.get(\"src\")\\n            if link and link.startswith(\"http\"):\\n                image_links.add(link)\\n\\n    # Create a directory for downloaded images\\n    if not os.path.exists(query):\\n        os.makedirs(query)\\n\\n    # Download the images\\n    for i, link in enumerate(image_links):\\n        try:\\n            response = requests.get(link)\\n            file = open(os.path.join(query, f\"{query}_{i+1}.jpg\"), \"wb\")\\n            file.write(response.content)\\n            file.close()\\n        except Exception as e:\\n            print(f\"Error: {e}\")\\n            continue\\n    \\n    print(f\"Downloaded {len(image_links)} images for {query}.\")\\n    return\\n\\n\\nquery = \"squirtle in game\"  # Introducir el término aquí.\\n\\ndownload_all_images(query)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la búsqueda de imágenes en Google imágenes (descomentar si se quiere probar)\n",
    "\n",
    "# Importamos las librerías necesarias\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "\n",
    "# Función para la descarga de imágenes\n",
    "def download_all_images(query):\n",
    "    image_links = set()\n",
    "\n",
    "    # Searching for the query in Google Images\n",
    "    search_query = query + \" images\"\n",
    "    for j in search(search_query, num=10, stop=10, pause=2):\n",
    "        if 'https://encrypted-tbn0.gstatic.com/images' in j:\n",
    "            continue\n",
    "        page = requests.get(j)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # Extracting image links\n",
    "        for raw_img in soup.find_all(\"img\"):\n",
    "            link = raw_img.get(\"src\")\n",
    "            if link and link.startswith(\"http\"):\n",
    "                image_links.add(link)\n",
    "\n",
    "    # Create a directory for downloaded images\n",
    "    if not os.path.exists(query):\n",
    "        os.makedirs(query)\n",
    "\n",
    "    # Download the images\n",
    "    for i, link in enumerate(image_links):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            file = open(os.path.join(query, f\"{query}_{i+1}.jpg\"), \"wb\")\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Downloaded {len(image_links)} images for {query}.\")\n",
    "    return\n",
    "\n",
    "\n",
    "query = \"squirtle in game\"  # Introducir el término aquí.\n",
    "\n",
    "download_all_images(query)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargadas las imágenes, vamos a escribir un nombre en cada imagen igual, para que después sea más sencillo el preprocesado de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Importar librerías\\nimport os\\n\\npath = 'pokemon'\\n\\n# Recorremos las imágenes de la carpeta y vamos cambiando el nombre\\nfor pokemon in os.listdir(path):\\n\\ti = 0\\n\\tfor filename in os.listdir(path+'/'+pokemon):\\n\\t\\tos.rename(path+'/'+pokemon+'/'+filename, path+'/'+pokemon+'/'+pokemon+'__'+str(i)+'.png')\\n\\t\\ti += 1\\n    \\nprint('Nombres cambiados')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para cambiar el nombre de las imágenes descargadas (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "# Importar librerías\n",
    "import os\n",
    "\n",
    "path = 'pokemon'\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el nombre\n",
    "for pokemon in os.listdir(path):\n",
    "\ti = 0\n",
    "\tfor filename in os.listdir(path+'/'+pokemon):\n",
    "\t\tos.rename(path+'/'+pokemon+'/'+filename, path+'/'+pokemon+'/'+pokemon+'__'+str(i)+'.png')\n",
    "\t\ti += 1\n",
    "    \n",
    "print('Nombres cambiados')\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo esto, obtuvimos muchas imágenes para cada Pokémon. Sin embargo, decidimos quedarnos con los Pokémon que más imágenes tenían, debido a que consideramos que es mejor obtener un buen modelo que funcione para una cantidad reducida de especies, que crear un modelo que funcione para muchas especies pero no sea demasiado preciso. <br>\n",
    "\n",
    "Por ello, los Pokémon que decidimos utilizar fueron los siguientes: Pikachu, Charmander, Charizard, Caterpie, Magikarp, Ratata, Geodude, Machop, Squirtle, Bulbasaur, Mew, Dragonite, Meowth, Lapras, Snorlax, Greninja, Rayquaza, Lucario, MrMime y Gengar (20 especies). <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesado de los datos\n",
    "Debido a que utilizamos diferentes filtros en la búsqueda de imágenes para obtener las máximas posibles, nos salieron muchas imágenes duplicadas o que no se correspondían con el Pokémon que queríamos. <br>\n",
    "\n",
    "Es por ello que debíamos hacer un preprocesado de los datos, para quedarnos con el conjunto de imágenes de cada Pokémon lo más limpio y homogéneo posible. <br>\n",
    "\n",
    "Esto conyevó varias tareas: \n",
    "- Eliminación de imágenes no correspondientes al Pokémon que queríamos. Para ello simplemente una vez descargadas todas las imágenes, nos metíamos en la carpeta e íbamos eliminando una a una las imágenes que no pertenecían a dicho Pokémon.\n",
    "- Eliminación de imágenes duplicadas. En este punto nos dimos cuenta de que hacerlo a mano iba a ser prácticamente imposible, por ello decidimos crear un pequeño script que comparase las imágenes entre sí y eliminase aquellas que fueran iguales, usando la libería Pilow.\n",
    "- Normalizar, redimensionar y convertir las imágenes a escala de grises. Para ello, utilizaremos keras.preprocessing.image, que nos permite realizar estas tareas de forma muy sencilla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Scripts usados para el preprocesdo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nimport os\\n\\n# Ruta de la carpeta con las imágenes\\ncarpeta = \\'pokemon/Meowth\\'\\n\\n# Diccionario para almacenar las imágenes idénticas\\nimagenes_ident = {}\\n\\n# Recorriendo la carpeta y comparando cada imagen con las demás\\nfor nombre_imagen1 in os.listdir(carpeta):\\n    ruta_imagen1 = os.path.join(carpeta, nombre_imagen1)\\n    if os.path.isfile(ruta_imagen1):\\n        imagen1 = Image.open(ruta_imagen1)\\n        for nombre_imagen2 in os.listdir(carpeta):\\n            ruta_imagen2 = os.path.join(carpeta, nombre_imagen2)\\n            if os.path.isfile(ruta_imagen2) and ruta_imagen1 != ruta_imagen2:\\n                imagen2 = Image.open(ruta_imagen2)\\n                if imagen1.size == imagen2.size and list(imagen1.getdata()) == list(imagen2.getdata()):\\n                    if nombre_imagen1 not in imagenes_ident:\\n                        imagenes_ident[nombre_imagen1] = [nombre_imagen2]\\n                    else:\\n                        imagenes_ident[nombre_imagen1].append(nombre_imagen2)\\n\\n# Mostrar las imágenes idénticas encontradas\\nfor imagen, imagenes_iguales in imagenes_ident.items():\\n    print(f\"La imagen {imagen} es idéntica a: {\\', \\'.join(imagenes_iguales)}\")\\n    \\n# Sacamos la imagen duplicada de la carpeta y la movemos a otra carpeta, dejando solamente una copia de la imagen\\nfor imagen, imagenes_iguales in imagenes_ident.items():\\n    # si no existe la carpeta la creamos\\n    if not os.path.exists(\\'pokemon/duplicadas\\'):\\n        os.makedirs(\\'pokemon/duplicadas\\')\\n    # movemos la imagen a la carpeta duplicadas\\n    os.rename(carpeta + \\'/\\' + imagen, \\'pokemon/duplicadas/\\' + imagen)\\n    # borramos las imagenes duplicadas\\n    for imagen_duplicada in imagenes_iguales:\\n        os.remove(carpeta + \\'/\\' + imagen_duplicada)\\n     \\n    \\n# Si no hay imágenes idénticas, mostrar un mensaje\\nif len(imagenes_ident) == 0:\\n    print(\"No hay imágenes idénticas.\")\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script de eliminación de imágenes duplicadas (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Ruta de la carpeta con las imágenes\n",
    "carpeta = 'pokemon/Meowth'\n",
    "\n",
    "# Diccionario para almacenar las imágenes idénticas\n",
    "imagenes_ident = {}\n",
    "\n",
    "# Recorriendo la carpeta y comparando cada imagen con las demás\n",
    "for nombre_imagen1 in os.listdir(carpeta):\n",
    "    ruta_imagen1 = os.path.join(carpeta, nombre_imagen1)\n",
    "    if os.path.isfile(ruta_imagen1):\n",
    "        imagen1 = Image.open(ruta_imagen1)\n",
    "        for nombre_imagen2 in os.listdir(carpeta):\n",
    "            ruta_imagen2 = os.path.join(carpeta, nombre_imagen2)\n",
    "            if os.path.isfile(ruta_imagen2) and ruta_imagen1 != ruta_imagen2:\n",
    "                imagen2 = Image.open(ruta_imagen2)\n",
    "                if imagen1.size == imagen2.size and list(imagen1.getdata()) == list(imagen2.getdata()):\n",
    "                    if nombre_imagen1 not in imagenes_ident:\n",
    "                        imagenes_ident[nombre_imagen1] = [nombre_imagen2]\n",
    "                    else:\n",
    "                        imagenes_ident[nombre_imagen1].append(nombre_imagen2)\n",
    "\n",
    "# Mostrar las imágenes idénticas encontradas\n",
    "for imagen, imagenes_iguales in imagenes_ident.items():\n",
    "    print(f\"La imagen {imagen} es idéntica a: {', '.join(imagenes_iguales)}\")\n",
    "    \n",
    "# Sacamos la imagen duplicada de la carpeta y la movemos a otra carpeta, dejando solamente una copia de la imagen\n",
    "for imagen, imagenes_iguales in imagenes_ident.items():\n",
    "    # si no existe la carpeta la creamos\n",
    "    if not os.path.exists('pokemon/duplicadas'):\n",
    "        os.makedirs('pokemon/duplicadas')\n",
    "    # movemos la imagen a la carpeta duplicadas\n",
    "    os.rename(carpeta + '/' + imagen, 'pokemon/duplicadas/' + imagen)\n",
    "    # borramos las imagenes duplicadas\n",
    "    for imagen_duplicada in imagenes_iguales:\n",
    "        os.remove(carpeta + '/' + imagen_duplicada)\n",
    "     \n",
    "    \n",
    "# Si no hay imágenes idénticas, mostrar un mensaje\n",
    "if len(imagenes_ident) == 0:\n",
    "    print(\"No hay imágenes idénticas.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Importar librerías\\nimport os\\nfrom PIL import Image\\n\\n# Seleccionamos la carpeta con las imágenes a normalizar\\n\\npath = \\'pokemon\\'\\n\\n# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\\nfor pokemon in os.listdir(path):\\n\\ti = 0\\n\\tfor filename in os.listdir(path+\\'/\\'+pokemon):\\n\\t\\t# Normalizar\\n\\t\\tif (filename[-3:] == \\'svg\\'):\\n\\t\\t\\tos.remove(path+\\'/\\'+pokemon+\\'/\\'+filename)\\n\\t\\telse:\\n\\t\\t\\timg = Image.open(path+\\'/\\'+pokemon+\\'/\\'+filename)\\n\\t\\t\\timg = img.resize((128,128))\\n\\t\\t\\tos.remove(path+\\'/\\'+pokemon+\\'/\\'+filename)\\n\\t\\t\\timg.save(path+\\'/\\'+pokemon+\\'/\\'+pokemon+\\'_\\'+str(i)+\\'.png\\')\\n\\t\\t\\ti += 1\\n\\t\\t\\nprint(\\'Imágenes normalizadas\\')\\n# Eliminar todos los jpg para evitar duplicados en varios formatos\\n\\nfor filename in os.listdir(path):\\n    if filename.endswith(\".jpg\"):\\n        os.remove(path+\\'/\\'+filename)\\n        \\nprint(\\'Imágenes jpg eliminadas\\')\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la normalización (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Seleccionamos la carpeta con las imágenes a normalizar\n",
    "\n",
    "path = 'pokemon'\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\n",
    "for pokemon in os.listdir(path):\n",
    "\ti = 0\n",
    "\tfor filename in os.listdir(path+'/'+pokemon):\n",
    "\t\t# Normalizar\n",
    "\t\tif (filename[-3:] == 'svg'):\n",
    "\t\t\tos.remove(path+'/'+pokemon+'/'+filename)\n",
    "\t\telse:\n",
    "\t\t\timg = Image.open(path+'/'+pokemon+'/'+filename)\n",
    "\t\t\timg = img.resize((128,128))\n",
    "\t\t\tos.remove(path+'/'+pokemon+'/'+filename)\n",
    "\t\t\timg.save(path+'/'+pokemon+'/'+pokemon+'_'+str(i)+'.png')\n",
    "\t\t\ti += 1\n",
    "\t\t\n",
    "print('Imágenes normalizadas')\n",
    "# Eliminar todos los jpg para evitar duplicados en varios formatos\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        os.remove(path+'/'+filename)\n",
    "        \n",
    "print('Imágenes jpg eliminadas')\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Importar librerías\\nimport os\\nimport cv2\\nimport numpy as np\\n\\n# Seleccionamos la carpeta con las imágenes a cambiar\\npath = 'pokemon'\\n\\n# Recorremos las imágenes de la carpeta y vamos aplicando el filtro\\nfor pokemon in os.listdir(path):\\n\\ti = 0\\n\\tfor filename in os.listdir(path+'/'+pokemon):\\n\\t\\t# Normalizar\\n\\t\\timg = cv2.imread(path+'/'+pokemon+'/'+filename)\\n\\t\\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\t\\tos.remove(path+'/'+pokemon+'/'+filename)\\n\\t\\tcv2.imwrite(path+'/'+pokemon+'/'+pokemon+'_'+str(i)+'.png',gray)\\n\\t\\ti += 1\\n    \\nprint('Imágenes en escala de grises')\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script para la conversión a escala de grises (descomentar si se quiere probar)\n",
    "\n",
    "\"\"\"\n",
    "# Importar librerías\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Seleccionamos la carpeta con las imágenes a cambiar\n",
    "path = 'pokemon'\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos aplicando el filtro\n",
    "for pokemon in os.listdir(path):\n",
    "\ti = 0\n",
    "\tfor filename in os.listdir(path+'/'+pokemon):\n",
    "\t\t# Normalizar\n",
    "\t\timg = cv2.imread(path+'/'+pokemon+'/'+filename)\n",
    "\t\tgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\tos.remove(path+'/'+pokemon+'/'+filename)\n",
    "\t\tcv2.imwrite(path+'/'+pokemon+'/'+pokemon+'_'+str(i)+'.png',gray)\n",
    "\t\ti += 1\n",
    "    \n",
    "print('Imágenes en escala de grises')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos anteriormente, a pesar de haber realizado una búsqueda masiva de imágenes, creemos que con apenas 400-1000 imágenes de cada especie no son suficientes, es por ello que vamos a utilizar técnicas como la rotación o volteo y así obtener, como mínimo, 2000 imágenes de cada especie. Para ello, podemos hacerlo de dos maneras:\n",
    "1. Realizarlo directamente sobre el conjunto de imágenes.\n",
    "2. Realizar la división del conjunto de imágenes en train y test y aplicar las técnicas de rotación y volteo sobre el conjunto de train.\n",
    "\n",
    "Cualquiera de las opciones debería ser correcta, pero hemos decidido realizar la segunda opción, ya que así podemos usar directamente el conjunto de train a la hora de crear el modelo y no tendríamos que hacer una división posteriormente, es decir, nos ahorra un paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Generamos 3000 imágenes de cada especie a partir de las 400-1000 que teníamos.\\n\\n# Importamos librerías\\nfrom keras.preprocessing.image import ImageDataGenerator\\nimport random\\n\\n# Generador de imágenes\\ngenerador_imagenes = ImageDataGenerator(rotation_range=10, # Rotación aleatoria de la imagen\\n                                        width_shift_range=0.1, # Desplazamiento horizontal aleatorio\\n                                        height_shift_range=0.1, # Desplazamiento vertical aleatorio\\n                                        zoom_range=0.1, # Zoom aleatorio\\n                                        horizontal_flip=True) # Volteo horizontal\\n\\n# Generamos 50 imágenes de cada imagen que tenemos y se guardan en la carpeta de la clase correspondiente\\nfor elemento in clases:\\n    # Recorremos las imágenes de cada clase\\n    for filename in os.listdir(path+'/'+elemento):\\n        # Añadimos la imagen a la lista X\\n        img = cv2.imread(path+'/'+elemento+'/'+filename)\\n        # Revisar que todas las imágenes tienen el mismo tamaño\\n        img = cv2.resize(img, (128,128))\\n        # Cambiamos la dimensión de la imagen\\n        img = img.reshape([-1, 128, 128, 3])\\n        # Generamos las imágenes\\n        generador_imagenes.fit(img)\\n        imagenes_generadas = generador_imagenes.flow(img, batch_size=50, save_to_dir=path+'/'+elemento, save_prefix='aug', save_format='png')\\n        # Guardamos las imágenes\\n        for i in range(10):\\n            imagenes_generadas.next()\\n            \\nprint('Imágenes generadas')\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generamos más imágenes (descomentar si se quiere probar)\n",
    "\"\"\"\n",
    "# Generamos 3000 imágenes de cada especie a partir de las 400-1000 que teníamos.\n",
    "\n",
    "# Importamos librerías\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "\n",
    "# Generador de imágenes\n",
    "generador_imagenes = ImageDataGenerator(rotation_range=10, # Rotación aleatoria de la imagen\n",
    "                                        width_shift_range=0.1, # Desplazamiento horizontal aleatorio\n",
    "                                        height_shift_range=0.1, # Desplazamiento vertical aleatorio\n",
    "                                        zoom_range=0.1, # Zoom aleatorio\n",
    "                                        horizontal_flip=True) # Volteo horizontal\n",
    "\n",
    "# Generamos 50 imágenes de cada imagen que tenemos y se guardan en la carpeta de la clase correspondiente\n",
    "for elemento in clases:\n",
    "    # Recorremos las imágenes de cada clase\n",
    "    for filename in os.listdir(path+'/'+elemento):\n",
    "        # Añadimos la imagen a la lista X\n",
    "        img = cv2.imread(path+'/'+elemento+'/'+filename)\n",
    "        # Revisar que todas las imágenes tienen el mismo tamaño\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        # Cambiamos la dimensión de la imagen\n",
    "        img = img.reshape([-1, 128, 128, 3])\n",
    "        # Generamos las imágenes\n",
    "        generador_imagenes.fit(img)\n",
    "        imagenes_generadas = generador_imagenes.flow(img, batch_size=50, save_to_dir=path+'/'+elemento, save_prefix='aug', save_format='png')\n",
    "        # Guardamos las imágenes\n",
    "        for i in range(10):\n",
    "            imagenes_generadas.next()\n",
    "            \n",
    "print('Imágenes generadas')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, conseguimos tener un dataset lo más limpio posible y preparado para el siguiente punto: realización del modelo y entrenamiento <directorio ./Pokemon>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar con la creación del modelo, vamos a visualizar a través de gráficas las diferentes clases que tenemos en nuestro dataset, con el objetivo de ver el número de imágenes que tenemos para cada clase, si hay desbalanceo o no. Recordamos que como tenemos 20 pokémon, el número de clases será 20. <br>\n",
    "Para la realización de este modelo, nos hemos basado en páginas de referencia tales como: stackoverflow, tensorflow y keras; en los cuales hemos usado el código que nos proporcionan, realizando las modificaciones necesarias para adaptarlo a nuestro problema. <br>\n",
    "- TensorFlow: https://www.tensorflow.org/tutorials/images/classification\n",
    "- Keras: https://keras.io/examples/vision/image_classification_from_scratch/\n",
    "- Stackoverflow: usado para resolver dudas puntuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de clases:  151\n",
      "Número de imágenes por clase:  {'Abra': 935, 'Aerodactyl': 1176, 'Alakazam': 1112, 'Arbok': 1478, 'Arcanine': 1414, 'Articuno': 1308, 'Beedrill': 557, 'Bellsprout': 583, 'Blastoise': 653, 'Bulbasaur': 321, 'Butterfree': 696, 'Caterpie': 646, 'Chansey': 612, 'Charizard': 546, 'Charmander': 702, 'Charmeleon': 686, 'Clefable': 531, 'Clefairy': 629, 'Cloyster': 627, 'Cubone': 617, 'Dewgong': 701, 'Diglett': 525, 'Ditto': 516, 'Dodrio': 674, 'Doduo': 511, 'Dragonair': 657, 'Dragonite': 613, 'Dratini': 766, 'Drowzee': 625, 'Dugtrio': 684, 'Eevee': 378, 'Ekans': 549, 'Electabuzz': 559, 'Electrode': 701, 'Exeggcute': 594, 'Exeggutor': 715, 'Farfetchd': 651, 'Fearow': 500, 'Flareon': 600, 'Gastly': 532, 'Gengar': 373, 'Geodude': 712, 'Gloom': 612, 'Golbat': 716, 'Goldeen': 605, 'Golduck': 637, 'Golem': 675, 'Graveler': 611, 'Greninja': 323, 'Grimer': 682, 'Growlithe': 708, 'Gyarados': 697, 'Haunter': 660, 'Hitmonchan': 646, 'Hitmonlee': 693, 'Horsea': 647, 'Hypno': 643, 'Ivysaur': 569, 'Jigglypuff': 690, 'Jolteon': 680, 'Jynx': 619, 'Kabuto': 590, 'Kabutops': 681, 'Kadabra': 637, 'Kakuna': 688, 'Kangaskhan': 648, 'Kingler': 715, 'Koffing': 678, 'Krabby': 667, 'Lapras': 533, 'Lickitung': 688, 'Machamp': 723, 'Machoke': 544, 'Machop': 747, 'Magikarp': 451, 'Magmar': 617, 'Magnemite': 639, 'Magneton': 606, 'Mankey': 760, 'Marowak': 717, 'Meowth': 829, 'Metapod': 679, 'Mew': 691, 'Mewtwo': 730, 'Moltres': 653, 'MrMime': 291, 'Muk': 732, 'Nidoking': 714, 'Nidoqueen': 669, 'Nidorina': 623, 'Nidorino': 655, 'Ninetales': 764, 'Oddish': 688, 'Omanyte': 582, 'Omastar': 596, 'Onix': 676, 'Paras': 574, 'Parasect': 556, 'Persian': 601, 'Pidgeot': 696, 'Pidgeotto': 642, 'Pidgey': 759, 'Pikachu': 1190, 'Pinsir': 655, 'Poliwag': 685, 'Poliwhirl': 655, 'Poliwrath': 654, 'Ponyta': 684, 'Porygon': 576, 'Primeape': 725, 'Psyduck': 1038, 'Raichu': 732, 'Rapidash': 862, 'Raticate': 716, 'Rattata': 360, 'Rayquaza': 567, 'Rhydon': 619, 'Rhyhorn': 703, 'Sandshrew': 696, 'Sandslash': 751, 'Scyther': 698, 'Seadra': 687, 'Seaking': 676, 'Seel': 638, 'Shellder': 843, 'Slowbro': 662, 'Slowpoke': 634, 'Snorlax': 600, 'Spearow': 1160, 'Squirtle': 729, 'Starmie': 618, 'Staryu': 626, 'Tangela': 663, 'Tauros': 699, 'Tentacool': 601, 'Tentacruel': 590, 'Vaporeon': 713, 'Venomoth': 690, 'Venonat': 590, 'Venusaur': 669, 'Victreebel': 574, 'Vileplume': 703, 'Voltorb': 688, 'Vulpix': 709, 'Wartortle': 637, 'Weedle': 618, 'Weepinbell': 534, 'Weezing': 658, 'Wigglytuff': 686, 'Zapdos': 611, 'Zubat': 512}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Script para pre-visualizar las clases y el número de imágenes por clase (descomentar si se quiere probar)\n",
    "\n",
    "# Importar librerías\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionamos la carpeta con las diferentes clases\n",
    "path = 'pokemon'\n",
    "clases = os.listdir(path) # Lista con las clases\n",
    "print(\"Número total de clases: \", len(clases))\n",
    "\n",
    "# Creamos un diccionario para almacenar el número de imágenes por clase\n",
    "num_imagenes = {}\n",
    "for elemento in clases:\n",
    "    num_imagenes[elemento] = len(os.listdir(path+'/'+elemento))\n",
    "    \n",
    "print(\"Número de imágenes por clase: \", num_imagenes)\n",
    "\n",
    "# cogemos solo las 5 clases con más imágenes\n",
    "#num_imagenes = {k: v for k, v in sorted(num_imagenes.items(), key=lambda item: item[1], reverse=True)[:5]}\n",
    "#clases = list(num_imagenes.keys())\n",
    "#print(\"Número de clases: \", len(clases))\n",
    "#print(\"Número de imágenes por clase: \", num_imagenes)\n",
    "\n",
    "# Dibujamos la gráfica\n",
    "plt.bar(range(len(num_imagenes)), list(num_imagenes.values()), align='center')\n",
    "plt.xticks(range(len(num_imagenes)), list(num_imagenes.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes:  101391\n",
      "Número de etiquetas:  101391\n",
      "Forma de X_train:  (81112, 128, 128, 3)\n",
      "Forma de X_test:  (20279, 128, 128, 3)\n",
      "Forma de Y_train:  (81112,)\n",
      "Forma de Y_test:  (20279,)\n"
     ]
    }
   ],
   "source": [
    "# Definimos el Conjunto X e Y, donde X son las imágenes y Y las clases\n",
    "\n",
    "# Importar librerías\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "X = [] # Lista con las imágenes\n",
    "Y = [] # Lista de etiquetas\n",
    "\n",
    "# Recorremos las clases\n",
    "for elemento in clases:\n",
    "    # Recorremos las imágenes de cada clase\n",
    "    for filename in os.listdir(path+'/'+elemento):\n",
    "        # Añadimos la imagen a la lista X\n",
    "        img = cv2.imread(path+'/'+elemento+'/'+filename)\n",
    "        # Revisar que todas las imágenes tienen el mismo tamaño\n",
    "        img = cv2.resize(img, (128,128))\n",
    "        X.append(img)\n",
    "        Y.append(elemento)\n",
    "        \n",
    "# Imprimimos la longitud de las listas\n",
    "print(\"Número de imágenes: \", len(X))\n",
    "print(\"Número de etiquetas: \", len(Y))\n",
    "\n",
    "# Convertimos las listas a arrays de numpy\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    " \n",
    "# División del conjunto de datos en entrenamiento y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=22, stratify=Y, shuffle=True)\n",
    "print(\"Forma de X_train: \", X_train.shape)\n",
    "print(\"Forma de X_test: \", X_test.shape)\n",
    "print(\"Forma de Y_train: \", Y_train.shape)\n",
    "print(\"Forma de Y_test: \", Y_test.shape)\n",
    "\n",
    "# Codificación one hot de las etiquetas\n",
    "encoder = LabelEncoder() \n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "Y_test = encoder.transform(Y_test)\n",
    "Y_train = to_categorical(Y_train, num_classes=151)\n",
    "Y_test = to_categorical(Y_test, num_classes=151)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos antes, vamos a generar más imágenes a aprtir de técnicas como la rotación, modo espejo, zoom, etc. Para ello, vamos a utilizar la librería ImageDataGenerator de keras, que nos permite realizar estas técnicas de forma muy sencilla. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\\ngenerador_imagenes = ImageDataGenerator(rotation_range = 45, # Graus de rotación\\n                            zoom_range = 0.2, # Zoom a aplicar\\n                            horizontal_flip = True, # Volteo horizontal\\n                            width_shift_range = 0.15, # Desplazamiento horizontal\\n                            height_shift_range = 0.15, # Desplazamiento vertical\\n                            shear_range = 0.2) # Cizallamiento\\n\\ngenerador_imagenes.fit(X_train)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "generador_imagenes = ImageDataGenerator(rotation_range = 45, # Graus de rotación\n",
    "                            zoom_range = 0.2, # Zoom a aplicar\n",
    "                            horizontal_flip = True, # Volteo horizontal\n",
    "                            width_shift_range = 0.15, # Desplazamiento horizontal\n",
    "                            height_shift_range = 0.15, # Desplazamiento vertical\n",
    "                            shear_range = 0.2) # Cizallamiento\n",
    "\n",
    "generador_imagenes.fit(X_train)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construcción del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos ya el conjunto de datos dividido en train and test, todas las imágenes necesarias generadas y el dataset limpio y preparado, vamos a proceder a la creación del modelo CNN. <br>\n",
    "Para ello seguiremos utilizando la librerçia Keras, la cual nos parece muy útil para realizar esta construcción y, además, contiene muchos ejemplos reales y en los cuales podemos orientarnos a la hora de realizar nuestro modelo. <br>\n",
    "A la hora de su realización, podemos basarnos en varios tipos de enfoque: <br> \n",
    "1. Secuencial: se trata de una pila de capas de redes neuronales, en la cual cada capa tiene una entrada y una salida: https://keras.io/guides/sequential_model/ <br>\n",
    "2. Funcional: se trata de un modelo más complejo, en el cual podemos tener varias entradas y salidas, y además podemos tener capas compartidas: https://keras.io/guides/functional_api/<br>\n",
    "\n",
    "Nosotros hemos decidido utilizar el modelo secuencial, ya que consideramos que es más sencillo y más adecuado para nuestro problema dado que como lo que queremos hacer es que, dada una imagen se genere una interfaz que nos diga que Pokémon es, solo necesitamos de una entrada y una salida. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128, 128, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 64, 64, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64, 64, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 32, 32, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 32, 32, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 16, 16, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 16, 16, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               8389120   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 151)               38807     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9729047 (37.11 MB)\n",
      "Trainable params: 9725655 (37.10 MB)\n",
      "Non-trainable params: 3392 (13.25 KB)\n",
      "_________________________________________________________________\n",
      "Número de imágenes de entrenamiento:  81112\n",
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 4.8475 - accuracy: 0.0460\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10775, saving model to modelo_check_1000_151_20.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 1418s 558ms/step - loss: 4.8475 - accuracy: 0.0460 - val_loss: 4.2076 - val_accuracy: 0.1077\n",
      "Epoch 2/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 4.1013 - accuracy: 0.1226\n",
      "Epoch 2: val_accuracy improved from 0.10775 to 0.22195, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1410s 556ms/step - loss: 4.1013 - accuracy: 0.1226 - val_loss: 3.5271 - val_accuracy: 0.2220\n",
      "Epoch 3/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 3.4127 - accuracy: 0.2339\n",
      "Epoch 3: val_accuracy improved from 0.22195 to 0.32738, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1415s 559ms/step - loss: 3.4127 - accuracy: 0.2339 - val_loss: 2.9474 - val_accuracy: 0.3274\n",
      "Epoch 4/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 2.7390 - accuracy: 0.3528\n",
      "Epoch 4: val_accuracy improved from 0.32738 to 0.34548, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1406s 555ms/step - loss: 2.7390 - accuracy: 0.3528 - val_loss: 2.9114 - val_accuracy: 0.3455\n",
      "Epoch 5/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 2.2239 - accuracy: 0.4558\n",
      "Epoch 5: val_accuracy improved from 0.34548 to 0.59653, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1434s 566ms/step - loss: 2.2239 - accuracy: 0.4558 - val_loss: 1.6337 - val_accuracy: 0.5965\n",
      "Epoch 6/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 1.8882 - accuracy: 0.5236\n",
      "Epoch 6: val_accuracy improved from 0.59653 to 0.67183, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1452s 573ms/step - loss: 1.8882 - accuracy: 0.5236 - val_loss: 1.3101 - val_accuracy: 0.6718\n",
      "Epoch 7/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 1.5965 - accuracy: 0.5855\n",
      "Epoch 7: val_accuracy did not improve from 0.67183\n",
      "2534/2534 [==============================] - 1424s 562ms/step - loss: 1.5965 - accuracy: 0.5855 - val_loss: 1.8594 - val_accuracy: 0.5482\n",
      "Epoch 8/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 1.3815 - accuracy: 0.6344\n",
      "Epoch 8: val_accuracy improved from 0.67183 to 0.74096, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1422s 561ms/step - loss: 1.3815 - accuracy: 0.6344 - val_loss: 0.9983 - val_accuracy: 0.7410\n",
      "Epoch 9/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 1.2291 - accuracy: 0.6689\n",
      "Epoch 9: val_accuracy did not improve from 0.74096\n",
      "2534/2534 [==============================] - 1583s 625ms/step - loss: 1.2291 - accuracy: 0.6689 - val_loss: 1.0931 - val_accuracy: 0.7210\n",
      "Epoch 10/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 1.0948 - accuracy: 0.6990\n",
      "Epoch 10: val_accuracy improved from 0.74096 to 0.77154, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1723s 680ms/step - loss: 1.0948 - accuracy: 0.6990 - val_loss: 0.8577 - val_accuracy: 0.7715\n",
      "Epoch 11/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.9945 - accuracy: 0.7238\n",
      "Epoch 11: val_accuracy improved from 0.77154 to 0.77898, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1748s 690ms/step - loss: 0.9945 - accuracy: 0.7238 - val_loss: 0.8479 - val_accuracy: 0.7790\n",
      "Epoch 12/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.8916 - accuracy: 0.7501\n",
      "Epoch 12: val_accuracy improved from 0.77898 to 0.79861, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1742s 687ms/step - loss: 0.8916 - accuracy: 0.7501 - val_loss: 0.7479 - val_accuracy: 0.7986\n",
      "Epoch 13/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.7715\n",
      "Epoch 13: val_accuracy did not improve from 0.79861\n",
      "2534/2534 [==============================] - 1690s 667ms/step - loss: 0.8093 - accuracy: 0.7715 - val_loss: 0.8235 - val_accuracy: 0.7832\n",
      "Epoch 14/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.7572 - accuracy: 0.7839\n",
      "Epoch 14: val_accuracy improved from 0.79861 to 0.82154, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1477s 583ms/step - loss: 0.7572 - accuracy: 0.7839 - val_loss: 0.6845 - val_accuracy: 0.8215\n",
      "Epoch 15/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.7997\n",
      "Epoch 15: val_accuracy did not improve from 0.82154\n",
      "2534/2534 [==============================] - 1460s 576ms/step - loss: 0.6949 - accuracy: 0.7997 - val_loss: 1.0703 - val_accuracy: 0.7310\n",
      "Epoch 16/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.6596 - accuracy: 0.8081\n",
      "Epoch 16: val_accuracy improved from 0.82154 to 0.84225, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1435s 566ms/step - loss: 0.6596 - accuracy: 0.8081 - val_loss: 0.5974 - val_accuracy: 0.8423\n",
      "Epoch 17/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.6030 - accuracy: 0.8231\n",
      "Epoch 17: val_accuracy did not improve from 0.84225\n",
      "2534/2534 [==============================] - 1463s 577ms/step - loss: 0.6030 - accuracy: 0.8231 - val_loss: 0.6235 - val_accuracy: 0.8367\n",
      "Epoch 18/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.8281\n",
      "Epoch 18: val_accuracy did not improve from 0.84225\n",
      "2534/2534 [==============================] - 1516s 598ms/step - loss: 0.5757 - accuracy: 0.8281 - val_loss: 0.6462 - val_accuracy: 0.8329\n",
      "Epoch 19/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.8395\n",
      "Epoch 19: val_accuracy improved from 0.84225 to 0.85448, saving model to modelo_check_1000_151_20.hdf5\n",
      "2534/2534 [==============================] - 1728s 682ms/step - loss: 0.5371 - accuracy: 0.8395 - val_loss: 0.5670 - val_accuracy: 0.8545\n",
      "Epoch 20/20\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.8498\n",
      "Epoch 20: val_accuracy did not improve from 0.85448\n",
      "2534/2534 [==============================] - 1759s 694ms/step - loss: 0.5073 - accuracy: 0.8498 - val_loss: 0.5814 - val_accuracy: 0.8517\n"
     ]
    }
   ],
   "source": [
    "# Script de Construcción del modelo\n",
    "\n",
    "# Importar librerías\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "# Definimos el modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, padding = 'same', activation = 'relu', input_shape =(128, 128, 3), kernel_initializer = 'he_normal'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal', activation = 'relu'))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(151, activation = 'softmax'))\n",
    "# Mostramos un resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Printear todas las imagenes que usa el modelo (len)\n",
    "print(\"Número de imágenes de entrenamiento: \", len(X_train))\n",
    "\n",
    "checkpoint = ModelCheckpoint('modelo_check_1000_151_20.hdf5', verbose = 1, monitor = 'val_accuracy', save_best_only = True)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 20, validation_data = [X_test, Y_test],\n",
    "                             steps_per_epoch=len(X_train) // 32, callbacks = [checkpoint])\n",
    "\n",
    "model.save('modelo_1000_151_20.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.8988\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87618, saving model to modelo_check_1000_151_40.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sonia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 1748s 688ms/step - loss: 0.3287 - accuracy: 0.8988 - val_loss: 0.4929 - val_accuracy: 0.8762\n",
      "Epoch 2/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.9036\n",
      "Epoch 2: val_accuracy improved from 0.87618 to 0.87657, saving model to modelo_check_1000_151_40.hdf5\n",
      "2534/2534 [==============================] - 1473s 581ms/step - loss: 0.3185 - accuracy: 0.9036 - val_loss: 0.5102 - val_accuracy: 0.8766\n",
      "Epoch 3/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.9065\n",
      "Epoch 3: val_accuracy did not improve from 0.87657\n",
      "2534/2534 [==============================] - 1423s 561ms/step - loss: 0.3066 - accuracy: 0.9065 - val_loss: 0.6118 - val_accuracy: 0.8503\n",
      "Epoch 4/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.9084\n",
      "Epoch 4: val_accuracy improved from 0.87657 to 0.88466, saving model to modelo_check_1000_151_40.hdf5\n",
      "2534/2534 [==============================] - 1434s 566ms/step - loss: 0.2968 - accuracy: 0.9084 - val_loss: 0.4738 - val_accuracy: 0.8847\n",
      "Epoch 5/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9103\n",
      "Epoch 5: val_accuracy did not improve from 0.88466\n",
      "2534/2534 [==============================] - 1430s 564ms/step - loss: 0.2890 - accuracy: 0.9103 - val_loss: 0.4888 - val_accuracy: 0.8808\n",
      "Epoch 6/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9136\n",
      "Epoch 6: val_accuracy did not improve from 0.88466\n",
      "2534/2534 [==============================] - 1430s 564ms/step - loss: 0.2801 - accuracy: 0.9136 - val_loss: 0.4788 - val_accuracy: 0.8831\n",
      "Epoch 7/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9167\n",
      "Epoch 7: val_accuracy did not improve from 0.88466\n",
      "2534/2534 [==============================] - 1431s 565ms/step - loss: 0.2709 - accuracy: 0.9167 - val_loss: 0.4793 - val_accuracy: 0.8841\n",
      "Epoch 8/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.9172\n",
      "Epoch 8: val_accuracy did not improve from 0.88466\n",
      "2534/2534 [==============================] - 1492s 589ms/step - loss: 0.2718 - accuracy: 0.9172 - val_loss: 0.4899 - val_accuracy: 0.8826\n",
      "Epoch 9/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9211\n",
      "Epoch 9: val_accuracy did not improve from 0.88466\n",
      "2534/2534 [==============================] - 1733s 684ms/step - loss: 0.2534 - accuracy: 0.9211 - val_loss: 0.4917 - val_accuracy: 0.8846\n",
      "Epoch 10/10\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.9222\n",
      "Epoch 10: val_accuracy did not improve from 0.88466\n",
      "2534/2534 [==============================] - 1768s 698ms/step - loss: 0.2541 - accuracy: 0.9222 - val_loss: 0.6082 - val_accuracy: 0.8539\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# Cargar el modelo existente\n",
    "existing_model = load_model('modelo_1000_151_30.hdf5')\n",
    "\n",
    "# Crear un nuevo checkpoint para guardar el modelo después de las nuevas épocas\n",
    "new_checkpoint = ModelCheckpoint('modelo_check_1000_151_40.hdf5', verbose=1, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Compilar el modelo cargado\n",
    "existing_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo por 10 épocas adicionales\n",
    "existing_model.fit(X_train, Y_train, batch_size=32, epochs=10, validation_data=[X_test, Y_test],\n",
    "                   steps_per_epoch=len(X_train) // 32, callbacks=[new_checkpoint])\n",
    "\n",
    "# Guardar el modelo después de las nuevas épocas\n",
    "existing_model.save('modelo_1000_151_40.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Abra', 1: 'Aerodactyl', 2: 'Alakazam', 3: 'Arbok', 4: 'Arcanine', 5: 'Articuno', 6: 'Beedrill', 7: 'Bellsprout', 8: 'Blastoise', 9: 'Bulbasaur', 10: 'Butterfree', 11: 'Caterpie', 12: 'Chansey', 13: 'Charizard', 14: 'Charmander', 15: 'Charmeleon', 16: 'Clefable', 17: 'Clefairy', 18: 'Cloyster', 19: 'Cubone', 20: 'Dewgong', 21: 'Diglett', 22: 'Ditto', 23: 'Dodrio', 24: 'Doduo', 25: 'Dragonair', 26: 'Dragonite', 27: 'Dratini', 28: 'Drowzee', 29: 'Dugtrio', 30: 'Eevee', 31: 'Ekans', 32: 'Electabuzz', 33: 'Electrode', 34: 'Exeggcute', 35: 'Exeggutor', 36: 'Farfetchd', 37: 'Fearow', 38: 'Flareon', 39: 'Gastly', 40: 'Gengar', 41: 'Geodude', 42: 'Gloom', 43: 'Golbat', 44: 'Goldeen', 45: 'Golduck', 46: 'Golem', 47: 'Graveler', 48: 'Greninja', 49: 'Grimer', 50: 'Growlithe', 51: 'Gyarados', 52: 'Haunter', 53: 'Hitmonchan', 54: 'Hitmonlee', 55: 'Horsea', 56: 'Hypno', 57: 'Ivysaur', 58: 'Jigglypuff', 59: 'Jolteon', 60: 'Jynx', 61: 'Kabuto', 62: 'Kabutops', 63: 'Kadabra', 64: 'Kakuna', 65: 'Kangaskhan', 66: 'Kingler', 67: 'Koffing', 68: 'Krabby', 69: 'Lapras', 70: 'Lickitung', 71: 'Machamp', 72: 'Machoke', 73: 'Machop', 74: 'Magikarp', 75: 'Magmar', 76: 'Magnemite', 77: 'Magneton', 78: 'Mankey', 79: 'Marowak', 80: 'Meowth', 81: 'Metapod', 82: 'Mew', 83: 'Mewtwo', 84: 'Moltres', 85: 'MrMime', 86: 'Muk', 87: 'Nidoking', 88: 'Nidoqueen', 89: 'Nidorina', 90: 'Nidorino', 91: 'Ninetales', 92: 'Oddish', 93: 'Omanyte', 94: 'Omastar', 95: 'Onix', 96: 'Paras', 97: 'Parasect', 98: 'Persian', 99: 'Pidgeot', 100: 'Pidgeotto', 101: 'Pidgey', 102: 'Pikachu', 103: 'Pinsir', 104: 'Poliwag', 105: 'Poliwhirl', 106: 'Poliwrath', 107: 'Ponyta', 108: 'Porygon', 109: 'Primeape', 110: 'Psyduck', 111: 'Raichu', 112: 'Rapidash', 113: 'Raticate', 114: 'Rattata', 115: 'Rayquaza', 116: 'Rhydon', 117: 'Rhyhorn', 118: 'Sandshrew', 119: 'Sandslash', 120: 'Scyther', 121: 'Seadra', 122: 'Seaking', 123: 'Seel', 124: 'Shellder', 125: 'Slowbro', 126: 'Slowpoke', 127: 'Snorlax', 128: 'Spearow', 129: 'Squirtle', 130: 'Starmie', 131: 'Staryu', 132: 'Tangela', 133: 'Tauros', 134: 'Tentacool', 135: 'Tentacruel', 136: 'Vaporeon', 137: 'Venomoth', 138: 'Venonat', 139: 'Venusaur', 140: 'Victreebel', 141: 'Vileplume', 142: 'Voltorb', 143: 'Vulpix', 144: 'Wartortle', 145: 'Weedle', 146: 'Weepinbell', 147: 'Weezing', 148: 'Wigglytuff', 149: 'Zapdos', 150: 'Zubat'}\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "Bulbasaur\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "Bulbasaur\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "Charmander\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "Charmander\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "Charmander\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "Pikachu\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "Pikachu\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "Pikachu\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "Rayquaza\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path_input = 'input'\n",
    "path_output = 'output'\n",
    "\n",
    "# Recorremos las imágenes de la carpeta y vamos cambiando el tamaño\n",
    "i = 0\n",
    "for filename in os.listdir(path_input):\n",
    "    # Normalizar\n",
    "    img = Image.open(path_input+'/'+filename)\n",
    "    img = img.resize((128,128))\n",
    "    img.save(path_output+'/'+'prueba'+'_'+str(i)+'.png')\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(path_output):\n",
    "    # Normalizar\n",
    "    img = cv2.imread(path_output+'/'+filename)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(path_output+'/'+'prueba'+'_'+str(i)+'.png',gray)\n",
    "    i += 1\n",
    "\n",
    "clases_ordenadas = {}\n",
    "i = 0\n",
    "for pokemon in os.listdir('pokemon'):\n",
    "\tclases_ordenadas[i] = pokemon\n",
    "\ti += 1\n",
    "print(clases_ordenadas)\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(path_output):\n",
    "\timg = image.load_img('output/' + filename, target_size=(128, 128))\n",
    "\timg_array = image.img_to_array(img)\n",
    "\timg_array = np.expand_dims(img_array, axis=0)\n",
    "\tmodelo = load_model('modelo_1000_151_40.hdf5')\n",
    "\tprediccion = modelo.predict(img_array)\n",
    "\tindice_prediccion = np.argmax(prediccion)\n",
    "\tpokemon_predicho = clases_ordenadas[indice_prediccion]\n",
    "\tprint(pokemon_predicho)\n",
    "\ti += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21126de3de0b02901272bc1080c53c327c2c88e32dda9e696532b5930d739af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
